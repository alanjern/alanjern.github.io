[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alan Jern",
    "section": "",
    "text": "I‚Äôm an Associate Professor of Psychology and cognitive scientist at Rose-Hulman Institute of Technology.\nI use computational models and behavioral experiments to study how people think and reason. I‚Äôm primarily interested in social cognition: how people think about other people. I have also studied how people learn and use concepts, and how people revise their beliefs after seeing new evidence.\nI write about TV and psychology at Overthinking TV.\n\nEducation\nPh.D., Psychology, 2013\nCarnegie Mellon University\nB.S., Computer Science, 2007\nUCLA"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Are win streaks on Jeopardy becoming more common?\n\n\nAside from a recent outlier season, probably not.\n\n\n\n\nData Analysis\n\n\n\n\nA recent unprecedented run of winning streaks on Jeopardy has led some to wonder whether something about the show or the contestants has changed. But is there anything in the data to suggest that this is an actual trend? Looking at data from all 38 seasons points to the recent run of streaks more likely being an outlier than part of a trend.\n\n\n\n\n\n\nJun 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAn analysis of Reddit‚Äôs r/place\n\n\nMy favorite thing on the Internet this year was Reddit‚Äôs r/place, a collaborative art project that lasted several days. Analyzing the official data from the event released by Reddit, I found that, contrary to my expectations, activity wasn‚Äôt dominated by a small fraction of users and bots were relatively rare.\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTeaching a seminar-style applied psychology course\n\n\n\n\n\n\n\nTeaching\n\n\n\n\nIn the Winter 21-22 quarter, I taught a new undergraduate course about applications of psychology to everyday life. I share my syllabus and describe how it went.\n\n\n\n\n\n\nMar 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWhen are waits lowest at Disney World?\n\n\nAvoid holidays, Fall is generally good, and never go on New Year‚Äôs Eve.\n\n\n\n\nData Analysis\n\n\n\n\nUsing data on posted wait times at Magic Kingdom in Disney World spanning about five years, I looked for seasonal patterns in average wait times. While there is considerable variability, there are clear patterns that you can take advantage of if you are flexible with your visit plans.\n\n\n\n\n\n\nDec 11, 2021\n\n\n\n\n\n\n  \n\n\n\n\nCan you save money at Union Hospital by not using insurance?\n\n\nDepending on your insurance plan, maybe yes, for some procedures.\n\n\n\n\nData Analysis\n\n\n\n\nDue to a new federal law, hospitals must publish their price lists. The price list for Union Hospital in Terre Haute shows what many other price lists from hospitals around the country do ‚Äì that insurers‚Äô negotiated prices sometimes don‚Äôt make sense.\n\n\n\n\n\n\nAug 27, 2021\n\n\n\n\n\n\n  \n\n\n\n\nThree police offers have died in Terre Haute in the last 10 years. Is that a lot?\n\n\nData suggests it is, relative to Terre Haute‚Äôs population.\n\n\n\n\nData Analysis\n\n\n\n\nA THPD detective was shot and killed this week, making it the third officer death in Terre Haute, Indiana in the past 10 years. I compared officer death counts to find out if this is a large number given Terre Haute‚Äôs relatively small size.\n\n\n\n\n\n\nJul 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\nTeaching research methods using formr\n\n\nA great alternative to Qualtrics for people at schools without access.\n\n\n\n\nTeaching\n\n\n\n\nIn the Spring 2021 term, I taught a research methods course in which students learned to build online studies using formr. I explain what worked, what didn‚Äôt, and why I recommend using formr to others.\n\n\n\n\n\n\nJun 17, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "(*) denotes student co-author."
  },
  {
    "objectID": "publications/publications.html#journal-articles",
    "href": "publications/publications.html#journal-articles",
    "title": "Publications",
    "section": "Journal articles",
    "text": "Journal articles\nA. Jern, *A. Derrow-Pinion, & *AJ Piergiovanni. (2021). A computational framework for understanding the roles of simplicity and rational support in people‚Äôs behavior explanations. Cognition. Blog post. Code, data, and materials.\nA. Jern. (2018). People are intuitive economists under the right conditions. Behavioral and Brain Sciences.\nA. Jern. (2018). A preliminary study of the educational benefits of conducting replications in the classroom. Scholarship of Teaching and Learning in Psychology. Data and materials.\nA. Jern, C. G. Lucas, & C. Kemp. (2017). People learn other people‚Äôs preferences through inverse decision-making. Cognition. Correction. Blog post. Code, data, and materials.\nA. Jern & C. Kemp. (2015). A decision network account of reasoning about other people‚Äôs choices. Cognition. Video abstract. Code, data, and materials.\nC. Kemp & A. Jern. (2014). A taxonomy of inductive problems. Psychonomic Bulletin & Review.\nA. Jern, K. K. Chang, & C. Kemp (2014). Belief polarization is not always rational. Psychological Review. Code and data.\nA. Jern & C. Kemp. (2013). A probabilistic account of exemplar and category generation. Cognitive Psychology. Code and data."
  },
  {
    "objectID": "publications/publications.html#conference-papers",
    "href": "publications/publications.html#conference-papers",
    "title": "Publications",
    "section": "Conference papers",
    "text": "Conference papers\n*S. Payne & A. Jern. (2022). Do people use social information to improve predictions about everyday events?. Proceedings of the 44th Annual Conference of the Cognitive Science Society. Code, data, and materials.\n*AJ Piergiovanni & A. Jern. (2015). Computational principles underlying people‚Äôs behavior explanations. Proceedings of the 37th Annual Conference of the Cognitive Science Society. Code and data.\nA. Jern & C. Kemp. (2014). Reasoning about social choices and social relationships. Proceedings of the 36th Annual Conference of the Cognitive Science Society.\nA. Jern, C. G. Lucas, & C. Kemp. (2011). Evaluating the inverse decision-making approach to preference learning. Advances in Neural Information Processing Systems 24.\nA. Jern & C. Kemp. (2011). Capturing mental state reasoning with influence diagrams. Proceedings of the 33rd Annual Conference of the Cognitive Science Society.\nC. Kemp, F. Han, & A. Jern. (2011). Concept learning and modal reasoning. Proceedings of the 33rd Annual Conference of the Cognitive Science Society.\nA. Jern & C. Kemp. (2011). Decision factors that support preference learning. Proceedings of the 33rd Annual Conference of the Cognitive Science Society.\nC. Kemp & A. Jern. (2009). Abstraction and relational learning. Advances in Neural Information Processing Systems 22.\nA. Jern, K. K. Chang, & C. Kemp. (2009). Bayesian belief polarization. Advances in Neural Information Processing Systems 22. Supporting material.\nC. Kemp, A. Jern, & F. Xu. (2009). Object discovery and identification. Advances in Neural Information Processing Systems 22.\nC. Kemp & A. Jern. (2009). A taxonomy of inductive problems. Proceedings of the 31st Annual Conference of the Cognitive Science Society.\nA. Jern & C. Kemp. (2009). Category generation. Proceedings of the 31st Annual Conference of the Cognitive Science Society."
  },
  {
    "objectID": "publications/publications.html#popular-science-articles",
    "href": "publications/publications.html#popular-science-articles",
    "title": "Publications",
    "section": "Popular science articles",
    "text": "Popular science articles\nDeepfake Luke Skywalker should scare us. Nautilus. March 2, 2022.\nThe intelligent life of droids. Nautilus. March 3, 2021.\nEffective altruism is logical, but too unnatural to catch on. Psyche. October 13, 2020.\nCovid-19 death skepticism, explained by a cognitive scientist. Vox. September 1, 2020.\nHBO‚Äôs My Brilliant Friend: A glance into the reality of child prodigies. Culturico. April 30, 2020.\nCould stabbing yourself help you remember? The cognitive psychology behind a scene from Netflix‚Äôs Altered Carbon. cogbites. March 16, 2020.\nWhat HBO‚Äôs Westworld gets wrong (and right) about human nature. The Conversation. November 3, 2016.\nEnough with the spoiler alerts! Plot spoilers often increase enjoyment. The Conversation. July 14, 2016."
  },
  {
    "objectID": "lab/lab.html",
    "href": "lab/lab.html",
    "title": "The Computational Social Cognition Lab",
    "section": "",
    "text": "In the Computational Social Cognition Lab, we mostly study how people think about other people. We do this using mathematical, computational, and experimental methods.\nI‚Äôm not currently taking on new research assistants at the moment.\nHere are some photos of past lab members presenting their work.\n\n\n\n\nStephen Payne, '22\n\n\n\n\n\nAnna Scott, '18\n\n\n\n\n\n\n\nAustin Derrow-Pinion, '18\n\n\n\n\n\nNathan Blank, '17"
  },
  {
    "objectID": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html",
    "href": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html",
    "title": "When are waits lowest at Disney World?",
    "section": "",
    "text": "Image credit: Jayme McColgan on Unsplash.\nI love Disney World. I hate waiting in line. Sadly you can‚Äôt go to Disney World without waiting in line. But if you plan ahead, you can keep the waiting to a minimum.\nPart of that planning is going at the right time. Visitors are somewhat predictable: they don‚Äôt visit uniformly throughout the year. If you have the flexibility, you can minimize your wait times by going when other people aren‚Äôt. But when are those times?\nTo try to answer that question, I found a great dataset from Touring Plans of posted wait times for several attractions in each park every few minutes of each day going back to 2015."
  },
  {
    "objectID": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#a-year-at-magic-kingdom-in-waits",
    "href": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#a-year-at-magic-kingdom-in-waits",
    "title": "When are waits lowest at Disney World?",
    "section": "A year at Magic Kingdom in waits",
    "text": "A year at Magic Kingdom in waits\nFor Magic Kingdom, Touring Plans has data for three attractions: Pirates of the Caribbean, Seven Dwarfs Mine Train, and Splash Mountain. To get a sense of how waits at Magic Kingdom vary over the calender year, I took waits from every day between January 1st, 2015 and March 15, 2020 (the last day Disney World was open before it shut down for Covid), dropped the year (combining, for example, all waits from January 1st from all years), then computed the mean waits for each day of the year based on the three attractions.\nHere are the results.\n\n\n\n\n\nThe plot also shows US holidays (some holidays fell on different days between 2015 and 2021), and estimates of public school breaks. School breaks vary a lot, so I just went with the dates of the Orange County Public School district in Florida, which includes Orlando. The data on US holidays comes from here.\nThere are a few conclusions that you can draw from this plot:\n\nWaits spike during holidays.\nWaits rise in the summer.\nWaits rise in late March/early April, probably due to Spring break.\nWaits seem to be lowest in the Fall and early December.\nOn any given day, waits vary a lot. The gray bands show the boundaries between the 25th and 75th percentiles for waits each day for the five years in the data. For example, the mean wait on January 1st is 52 minutes, but 25% of the posted waits on that day for the three attractions were under 20 minutes and 25% of the posted waits were over 75 minutes.\nNew Year‚Äôs Eve is hell at Magic Kingdom.\n\nThis plot is already pretty helpful, but it would be nice if we would be a little more precise. That is, can we use a statistical model to say, in quantitative terms, how much better or worse the best and worst times to visit are?"
  },
  {
    "objectID": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#how-much-of-a-difference-does-the-time-of-year-make",
    "href": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#how-much-of-a-difference-does-the-time-of-year-make",
    "title": "When are waits lowest at Disney World?",
    "section": "How much of a difference does the time of year make?",
    "text": "How much of a difference does the time of year make?\nI fit a linear regression model that used week of the year and whether the day was within a day of a holiday to predict wait times. (Including whether a day was during a school break didn‚Äôt make much difference in terms of predictions, probably because week of the year already captured most of what ‚Äúbreak time‚Äù would.)\n\nResults\nOn average, days that are on or near holidays have average waits that are about 6 minutes longer than days that aren‚Äôt.\nThe effects of week are shown below. Because the actual wait times are somewhat meaningless (they are just averages of three attractions), I‚Äôve plotted changes in terms of percent. These changes are relative to Week 16 of the year (around mid-April), which is pretty close to an average week at Disney World.\n\n\n\n\n\nThe best weeks to visit in terms of low waits are:\n\nSeptember through early October, when waits can decrease by 20-30% from baseline.\nfirst half of December, when waits can decrease by about 20% from baseline.\n\nThe worst weeks to visit are:\n\nThe last two weeks of the year and the first week of the year, when waits can increase by over 20% from baseline.\nLate March to early April, when waits can increase by about 20% from baseline.\nMid-July to early August, when waits can increase by 15-20% from baseline.\n\nAnd don‚Äôt ever go to Disney World on New Years Eve."
  },
  {
    "objectID": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#how-has-covid-affected-waits",
    "href": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#how-has-covid-affected-waits",
    "title": "When are waits lowest at Disney World?",
    "section": "How has Covid affected waits?",
    "text": "How has Covid affected waits?\nDisney World shut down due to Covid on March 15th, 2020. It reopened on July 11th that same year and has remained open ever since. It changed a lot of its operations after reopening. And, initially, visitors were skittish about returning. So how have wait times changed since reopening?\nTo answer this question, I compared the model‚Äôs predicted wait times to the posted wait times from the Touring Plans dataset, collected from the date of reopening.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\nNote that there‚Äôs a lot of missing data in the Fall of 2020.\nThis plot suggests that, since reopening, wait times have generally been a little lower than average, except for maybe in July 2021. However, it looks like waits are beginning to return to their usual patterns as of this past Fall. It‚Äôs probably too soon to say whether it‚Äôs a persisent pattern.\nFor comparison, we can do the same analysis for a different park. I fit another model using the same method with Touring Plans data for the Hollywood Studios park in Disney World. Touring Plans provides wait times for four attractions in Hollywood Studios: Rock ‚Äòn‚Äô Roller Coaster, Alien Swirling Saucers, Slinky Dog Dash, and Toy Story Mania.\nHere‚Äôs the comparable plot.\n\n\n\n\n\nThe pattern is basically the same. Generally lower-than-expected waits and an apparent return to normal starting this past Fall."
  },
  {
    "objectID": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#should-i-book-my-trip",
    "href": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#should-i-book-my-trip",
    "title": "When are waits lowest at Disney World?",
    "section": "Should I book my trip?",
    "text": "Should I book my trip?\nNot quite yet.\nAs a check, I compared my work to the weekly ranking developed by Dave Shute of yourfirstvisit.net. Dave doesn‚Äôt base his recommendations on statistics and data analysis. Instead, they‚Äôre based on his years of firsthand knowledge and experience. Here are his recommendations for 2021.\n Surprisingly, his recommendations don‚Äôt align with mine much at all. Some of his top-ranked weeks are in the middle of summer. After that, he recommends going in mid-April and early May.\n\nWhat‚Äôs going on?\nFirst, Dave factors in more than crowd levels. He considers things like weather and ride closures. September may be a low-crowd time but it‚Äôs also peak hurricane season. And low-crowd times tend to coincide with suboptimal experiences: Disney is well aware of these seasonal patterns, so they reduce operating hours and schedule attraction refurbishments in the off-season. This means you may get less time in the park and fewer things to do if you go during a less popular time.\nSo if your goal is to have the best overall experience (which Dave‚Äôs advice is targeted toward), choosing the the lowest-crowd time may not actually be best.\nSecond, Dave may have some knowledge not captured in the data. He seems to discount some of the weeks (like in early December) as heavy-crowd times that my analysis predicts will have lower than average waits.\n\n\nOkay, but all I care about is short waits. Should I book my trip now?\nAll else being equal, lower waits are better. But is this data giving an accurate picture of wait times? Maybe not.\nAside from the fact that the dataset only includes a few attractions per park, the data are posted wait times, not actual wait times. Disney is notorious for inflating posted wait times. After all, people are happier when they get in a line expecting to wait 30 minutes and only end up waiting 15 than when they get in a line expecting to wait 15 and end up waiting 30.\nAdditionally, ever since Disney parks started posting wait times on their mobile apps, it‚Äôs been strongly suspected that they intentionally manipulate posted wait times to disperse crowds. For example, they might inflate wait times in Tomorrowland to push people toward Frontierland on the other side of the park. (To learn more about this, along with the fascinating history of FastPasses and their influence on wait times, check out this nearly two-hour long video. It‚Äôs really interesting, I promise!)\nSo you probably shouldn‚Äôt take the absolute numbers too seriously. But if waits are consistently inflated year-round, you can probably expect to encounter shorter waits in September than in June, for example, and you can treat this data as an indirect measure of crowds levels."
  },
  {
    "objectID": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#conclusion",
    "href": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#conclusion",
    "title": "When are waits lowest at Disney World?",
    "section": "Conclusion",
    "text": "Conclusion\nThere is a clear seasonal pattern to waits (or, perhaps more accurately, crowd levels) at Disney World. Exactly how much impact this will have on the quality of your visit is hard to say because that depends on many more factors.\nBut, for the love of God, don‚Äôt go to Disney World on New Year‚Äôs Eve."
  },
  {
    "objectID": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#code-and-data",
    "href": "blog/posts/2021-12-11-disney-world-waits/wdwwaits.html#code-and-data",
    "title": "When are waits lowest at Disney World?",
    "section": "Code and data",
    "text": "Code and data\nData and the full analysis code I used to generate these plots are available here."
  },
  {
    "objectID": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html",
    "href": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html",
    "title": "Three police offers have died in Terre Haute in the last 10 years. Is that a lot?",
    "section": "",
    "text": "Image credit: Matt Popovich on Unsplash."
  },
  {
    "objectID": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html#cumulative-deaths-compared-to-population",
    "href": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html#cumulative-deaths-compared-to-population",
    "title": "Three police offers have died in Terre Haute in the last 10 years. Is that a lot?",
    "section": "Cumulative deaths compared to population",
    "text": "Cumulative deaths compared to population\nFirst, let‚Äôs just compare city/county population to the cumulative number of officer deaths since 1980.\n\npopVsDeaths &lt;- ggplot() +\n  geom_point(data = filter(deathCountsWithLabels, population &lt;= 75000),\n             mapping = aes(x = log10(population), y = numDeaths, shape = locationType),\n             color = \"turquoise3\", alpha = 1/3, show.legend = FALSE) +\n  geom_point(data = filter(deathCountsWithLabels, population &gt; 75000),\n             mapping = aes(x = log10(population), y = numDeaths, shape = locationType),\n             color = \"darkslategrey\", alpha = 1/3) +\n  geom_point(data = deathCountsHighlights,\n             mapping = aes(x = log10(population), y = numDeaths, shape = locationType),\n             color = \"orangered1\",\n             show.legend = FALSE) +\n  geom_text_repel(data = deathCountsWithLabels,\n                  mapping = aes(x = log10(population), y = numDeaths, label = locationLabel),\n                  size = 3,\n                  max.overlaps = Inf) +\n  scale_x_continuous(labels = c(\"100\", \"1,000\", \"10,000\", \"100,000\", \"1,000,000\", \"10,000,000\")) +\n  labs(x = \"Population\",\n       y = \"Deaths\",\n       shape = \"Location Type\",\n       title = str_c(\"Officer Deaths, \", startingYear, \"-\", finalYear)) +\n  guides(shape = guide_legend(override.aes = list(alpha = 1, color = \"black\")))\nprint(popVsDeaths)\n\n\n\n\n‚ö†Ô∏è One problem with this plot is that many departments showing zero deaths are actually mistakes. This is due to the overly simple department-location matching procedure I used.\nOne example is the Miami-Dade Police Department. This got included because it has the phrase Police Department in it. However, Miami-Dade is a county (a large one, with about 2.5 million people living there) and is identified as Miami-Dade County in my county population dataset. So the department name and county name fail to match because the word ‚Äúcounty‚Äù doesn‚Äôt appear in the name of the department.\nThis actually happened a lot, so you can‚Äôt really trust all the zeros. But surely some of the zeros are accurate. For example, this site reports that 16 out of Indiana‚Äôs 92 counties have had no ‚Äúline of duty‚Äù deaths. To not include departments with no deaths would be a mistake. But given the dataset I had, there wasn‚Äôt a reliable way to identify those departments, and I knew for a fact that many of the zeros I found were simply due to name mismatches. So I decided it was best overall to just remove the zeros entirely.\nHere‚Äôs the same plot with the zeros removed.\n\npopVsDeaths2 &lt;- ggplot() +\n  geom_point(data = filter(deathCountsWithLabels2, population &lt;= 75000),\n             mapping = aes(x = log10(population), y = numDeaths, shape = locationType),\n             color = \"turquoise3\", alpha = 1/3, show.legend = FALSE) +\n  geom_point(data = filter(deathCountsWithLabels2, population &gt; 75000),\n             mapping = aes(x = log10(population), y = numDeaths, shape = locationType),\n             color = \"darkslategrey\", alpha = 1/3) +\n  geom_point(data = deathCountsHighlights2,\n             mapping = aes(x = log10(population), y = numDeaths, shape = locationType),\n             color = \"orangered1\",\n             show.legend = FALSE) +\n  geom_text_repel(data = deathCountsWithLabels2,\n                  mapping = aes(x = log10(population), y = numDeaths, label = locationLabel),\n                  size = 3,\n                  max.overlaps = Inf) +\n  scale_x_continuous(labels = c(\"1,000\", \"10,000\", \"100,000\", \"1,000,000\", \"10,000,000\")) +\n  labs(x = \"Population\",\n       y = \"Deaths\",\n       shape = \"Location\",\n       title = str_c(\"Officer Deaths, \", startingYear, \"-\", finalYear),\n       subtitle = \"Locations where at least one death occurred\") +\n  guides(shape = guide_legend(override.aes = list(alpha = 1, color = \"black\")))\nprint(popVsDeaths2)\n\n\n\n\nMany fewer departments (especially from small cities), but same basic shape. In both plots, I‚Äôve colored small towns like Terre Haute in turquoise and also higlighted a few standout locations to help you orient yourself."
  },
  {
    "objectID": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html#scaling-by-population",
    "href": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html#scaling-by-population",
    "title": "Three police offers have died in Terre Haute in the last 10 years. Is that a lot?",
    "section": "Scaling by population",
    "text": "Scaling by population\nThese plots are a little hard to make sense of because the larger cities have way more deaths which squashes the plot vertically and makes it difficult to draw comparisons. So let‚Äôs try scaling the number of deaths by city/county population.\nNote: Arguably it would make more sense to scale by department size, but this information is harder to come by. I assume department size is correlated with population anyway.\nAlso, from this point on, I‚Äôm going to continue to exclude the zero-death locations for the reasons explained above.\n\nscaledPopsVsDeaths &lt;- ggplot(data = deathCountsNoZeros) +\n  geom_point(mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths),\n             shape = \"circle open\",\n             alpha = 1/3) +\n  geom_point(data = deathCountsHighlights2,\n             mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths),\n             shape = \"circle\",\n             color = \"coral3\",\n             show.legend = FALSE) +\n  geom_text_repel(data = deathCountsWithLabels2,\n                  mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths,\n                                label = locationLabel),\n                  size = 3,\n                  max.overlaps = Inf) +\n  scale_x_continuous(labels = c(\"1,000\", \"10,000\", \"100,000\", \"1,000,000\", \"10,000,000\")) +\n  labs(x = \"Population\",\n       y = \"Deaths per 10K\",\n       size = \"Total deaths\",\n       title = \"Officer deaths per 10,000 in the department's jurisdiction\",\n       subtitle = str_c(\"Cumulative deaths, \", startingYear, \"-\", finalYear)) +\n  theme(legend.position = \"bottom\")\n  \nprint(scaledPopsVsDeaths)\n\n\n\n\nThis plot suggests that Terre Haute is above average but maybe not a huge outlier, even among cities of similar population size.\nMore generally, this plot suggests there is not much of a relationship between city size and number of officer deaths, at least for cities above a certain size. The reason it looks like there is a relationship between population and deaths is almost certainly due to there being greater variability for smaller towns/departments. For example, a single death in a small town would have a big effect on this metric.\nSo let‚Äôs exclude small towns (population &lt; 50,000) to get a better understanding.\n\nscaledPopsVsDeaths_noSmallTowns &lt;- ggplot(data = deathCounts_noSmallTowns) +\n  geom_point(mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths),\n             shape = \"circle open\",\n             alpha = 1/3) +\n  geom_point(data = deathCountsHighlights_noSmallTowns,\n             mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths),\n             shape = \"circle\",\n             color = \"coral3\",\n             show.legend = FALSE) +\n  geom_text_repel(data = deathCountsWithLabels_noSmallTowns,\n                  mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths,\n                                label = locationLabel),\n                  size = 3) +\n  scale_x_continuous(breaks = c(4,5,6,7),\n                     labels = c(\"10,000\", \"100,000\", \"1,000,000\", \"10,000,000\")) +\n  labs(x = \"Population\",\n       y = \"Deaths per 10K\",\n       size = \"Total deaths\",\n       title = \"Officer deaths per 10,000 people in the department's jurisdiction\",\n       subtitle = str_c(\"Cumulative deaths, \", startingYear, \"-\", finalYear)) +\n  theme(legend.position = \"bottom\")\n  \nprint(scaledPopsVsDeaths_noSmallTowns)\n\n\n\n\nThis plot allows us to get a better sense of variability, especially among midsize towns. In this plot, Terre Haute does look a bit more like an outlier. In more quantitative terms, among the locations in the plot above, Terre Haute is in the 99th percentile for officer deaths, scaled by population size. It drops to the 93rd percentile if you include the small towns that were in the previous plot."
  },
  {
    "objectID": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html#officer-deaths-over-time",
    "href": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html#officer-deaths-over-time",
    "title": "Three police offers have died in Terre Haute in the last 10 years. Is that a lot?",
    "section": "Officer deaths over time",
    "text": "Officer deaths over time\nOne last question I had was whether Terre Haute‚Äôs outlier status was driven by the last 10 years. To get a sense of this, let‚Äôs look at officer deaths over time.\n\ndeathsOverTime &lt;- ggplot(data = deathCountsByYear) +\n  geom_step(mapping = aes(x = EOW, y = deathsPer10k, group = location_long),\n            alpha = 1/12) +\n  geom_step(data = deathCountsByYearHighlights,\n            mapping = aes(x = EOW, y = deathsPer10k, group = location_long),\n            color = \"gold2\",\n            alpha = 2/3) +\n  geom_step(data = filter(deathCountsByYear, location_long == \"Terre Haute, IN\"),\n            mapping = aes(x = EOW, y = deathsPer10k),\n            color = \"coral3\",\n            size = 1.5) +\n  geom_text_repel(data = filter(deathCountsByYearHighlights, EOW == today()),\n                  mapping = aes(x = EOW, y = deathsPer10k, group = location_long,\n                                label = location_long),\n                  size = 3) +\n  labs(title = \"Cumulative officer deaths\",\n       subtitle = \"Jurisdictions with populations &gt; 50,000\",\n       x = \"Date\",\n       y = \"Total deaths per 10,000 residents\") +\n  theme_minimal() +\n  theme(panel.grid.minor.x = element_blank(),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.y = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\nprint(deathsOverTime)\n\n\n\n\nThis plot shows cumulative deaths since 1980 for each department serving an area over 50,000 (again limited to only those departments that had at least one death). Terre Haute is highlighted. It gives the impression that Terre Haute wasn‚Äôt much of an outlier until the past 10 years when the last three officers died.\nIt also shows that no officers died in Terre Haute for several decades before 2011."
  },
  {
    "objectID": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html#code-and-data",
    "href": "blog/posts/2021-07-09-police-deaths-terre-haute/policedeaths.html#code-and-data",
    "title": "Three police offers have died in Terre Haute in the last 10 years. Is that a lot?",
    "section": "Code and data üìë",
    "text": "Code and data üìë\nData and the full analysis code I used to generate these plots are available here."
  },
  {
    "objectID": "blog/posts/2021-08-27-union-hospital-prices/unionhospital.html",
    "href": "blog/posts/2021-08-27-union-hospital-prices/unionhospital.html",
    "title": "Can you save money at Union Hospital by not using insurance?",
    "section": "",
    "text": "Image credit: Marcelo Leal on Unsplash.\nOne of the many frustrating things about health care in the US is not having a clue what things are going to cost. But this year, a new federal law went into effect requiring hospitals to publish all of their prices.\nI learned about this after reading this article in the New York Times by Sarah Kliff and Josh Katz. They analyzed the price lists at a number of hospitals and found some huge disparities in prices for identical treatments at the same hospitals and some puzzling discrepancies in insurers‚Äô negotiated prices. Notably, sometimes, the listed price for a treatment negotiated by an insurance company was higher than the listed price for a patient with no insurance at all. For insurance plans with high deductibles, this means you could potentially save money by not using your insurance for some treatments.\nThis got me curious about my own insurance and my own local hospital, so I decided to do a similar analysis."
  },
  {
    "objectID": "blog/posts/2021-08-27-union-hospital-prices/unionhospital.html#how-seriously-should-we-take-this-information",
    "href": "blog/posts/2021-08-27-union-hospital-prices/unionhospital.html#how-seriously-should-we-take-this-information",
    "title": "Can you save money at Union Hospital by not using insurance?",
    "section": "How seriously should we take this information?",
    "text": "How seriously should we take this information?\nIt‚Äôs probably hard to be sure. The price that gets charged for medical care surely depends on lots of factors. So I personally wouldn‚Äôt consider it a guarantee that because an entry in this list shows that a specific procedure is cheaper with no insurance that I‚Äôd be better off not using my insurance to get it.\nBut this information isn‚Äôt made up either. Hospitals do use these price lists as a starting point (at least) for billing. So I think it‚Äôs reasonable to use these price lists to draw some broadly valid conclusions about the value of your health insurance and the approximate costs of your health care."
  },
  {
    "objectID": "blog/posts/2021-08-27-union-hospital-prices/unionhospital.html#how-much-does-this-matter",
    "href": "blog/posts/2021-08-27-union-hospital-prices/unionhospital.html#how-much-does-this-matter",
    "title": "Can you save money at Union Hospital by not using insurance?",
    "section": "How much does this matter?",
    "text": "How much does this matter?\nOne last point I haven‚Äôt addressed is that the price charged by the hospital is not the same thing as the price you owe as an individual. The difference between a price tag of $50,000 and $60,000 may be irrelevant to you if you have health insurance with a deductible of $2,000 that pays all costs after that deductible is met. So some of these price discrepancies are problems for insurance companies, not for consumers.\nBut, as you‚Äôll note above, some of the cases where Anthem is beat by the ‚Äúno insurance‚Äù rates are well below common deductibles. So these cost differences can affect how much you are paying üíµ.\n\nCode and data üìë\nCode and data for this analysis is available here."
  },
  {
    "objectID": "blog/posts/2021-06-17-research-methods-formr/formr.html",
    "href": "blog/posts/2021-06-17-research-methods-formr/formr.html",
    "title": "Teaching research methods using formr",
    "section": "",
    "text": "Image credit: Greg Rosenke on Unsplash"
  },
  {
    "objectID": "blog/posts/2021-06-17-research-methods-formr/formr.html#the-course",
    "href": "blog/posts/2021-06-17-research-methods-formr/formr.html#the-course",
    "title": "Teaching research methods using formr",
    "section": "The course üéí",
    "text": "The course üéí\nIt‚Äôs useful to know a bit about my course first to help you decide if the information in this post will be useful to you.\nWhere I teach doesn‚Äôt have any psych majors and my course doesn‚Äôt have any psych pre-reqs. It‚Äôs a small school that doesn‚Äôt have a Qualtrics license. The students are mostly science and engineering majors so they are generally tech-savvy, but they don‚Äôt all have much programming experience.\nThe course itself is focused on replication ‚Äì students don‚Äôt design their own studies ‚Äì and I choose which studies they will be replicating. So I have control over the complexity of the study designs they‚Äôll be using.\n(You can see the course syllabus here.)\nSo what I was looking for was a platform for running online studies that was free, pretty easy to learn, and had enough base functionality to create some simple study designs."
  },
  {
    "objectID": "blog/posts/2021-06-17-research-methods-formr/formr.html#what-is-formr",
    "href": "blog/posts/2021-06-17-research-methods-formr/formr.html#what-is-formr",
    "title": "Teaching research methods using formr",
    "section": "What is formr? üõ†Ô∏è",
    "text": "What is formr? üõ†Ô∏è\n\n\n\nformr\n\n\nEnter formr. Compared to alternatives, formr best met all these criteria. Some other options to consider:\n\nQualtrics: Most behavioral scientists already know Qualtrics. It‚Äôs easy to use and super flexible. But it‚Äôs not free and I don‚Äôt have access to it where I work.\nPsyToolkit: I‚Äôve used PsyToolkit in the past. It‚Äôs fairly easy to learn, even for people with little programming experience. It‚Äôs probably a better choice over formr if you need to do more cognitive-type experiments where you control what people see on the screen at certain times and collect response time data. But I don‚Äôt really like the interface and it‚Äôs more advanced than what I needed.\njsPsych: Another good choice if you want more advanced options. But it would require teaching more programming than my course had time for."
  },
  {
    "objectID": "blog/posts/2021-06-17-research-methods-formr/formr.html#what-worked",
    "href": "blog/posts/2021-06-17-research-methods-formr/formr.html#what-worked",
    "title": "Teaching research methods using formr",
    "section": "What worked üëç",
    "text": "What worked üëç\nDespite the somewhat confusing documentation, students caught on to the basics of formr pretty quickly. To supplement the official documentation, I made the following tutorial that you are welcome to share1.\n\n\nAlthough I didn‚Äôt get any specific reports of this, another benefit of formr is that you initially build the experiment in Google Sheets, which was nice when students were working in groups because it was so easy for them to collaborate."
  },
  {
    "objectID": "blog/posts/2021-06-17-research-methods-formr/formr.html#what-didnt-work",
    "href": "blog/posts/2021-06-17-research-methods-formr/formr.html#what-didnt-work",
    "title": "Teaching research methods using formr",
    "section": "What didn‚Äôt work üôÑ",
    "text": "What didn‚Äôt work üôÑ\nThere were some issues with the formr.org website itself. I‚Äôm incredibly grateful to the formr team, but it‚Äôs a small operation and it‚Äôs probably not equipped to handle the rising interest formr has received over the past year or so. I believe this was responsible for the two main problems I encountered:\n\nIn order to make experiments on the formr.org website, you need an admin account, and new accounts have to be manually approved. After my students signed up, they weren‚Äôt approved for about 1.5 weeks. In the end, this wasn‚Äôt catastrophic, but our term is only 10 weeks, and it caused me to have to delay the deadline of the first formr-based assignment. Not ideal.\nOnce students were in the thick of collecting data for their final projects, I got multiple reports of the formr site going down. Also not good.\n\n\nThe solution?\nI learned that the formr team actually recommends installing your own instance of formr so you don‚Äôt have to rely on their website.\nI haven‚Äôt done this myself so I can‚Äôt say anything about it. But my plan for the next time I teach this course is to talk to the IT department at my school and see about installing formr on a college server. This should address the sign-up and downtime problems that we suffered this past term."
  },
  {
    "objectID": "blog/posts/2021-06-17-research-methods-formr/formr.html#tldr",
    "href": "blog/posts/2021-06-17-research-methods-formr/formr.html#tldr",
    "title": "Teaching research methods using formr",
    "section": "tl;dr üìî",
    "text": "tl;dr üìî\nformr is free, requires no set-up (if you run it from the existing website) and can be used to do most of what Qualtrics can do with minimal learning. (It can do a lot more advanced stuff too if you are willing to learn more.)\nI think it‚Äôs a great choice for research methods courses that want to offer students a simple solution for building good-looking ‚ú® and functional ‚öôÔ∏è online studies."
  },
  {
    "objectID": "blog/posts/2021-06-17-research-methods-formr/formr.html#footnotes",
    "href": "blog/posts/2021-06-17-research-methods-formr/formr.html#footnotes",
    "title": "Teaching research methods using formr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI learned after I made this video that the advice I give in it for randomizing subjects equally between conditions is maybe not the optimal method (though it might be the simplest). See here.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/posts/2022-03-03-applied-psych-course/appliedpsych.html",
    "href": "blog/posts/2022-03-03-applied-psych-course/appliedpsych.html",
    "title": "Teaching a seminar-style applied psychology course",
    "section": "",
    "text": "Image credit: Filip Urban on Unsplash.\nThis past quarter, I taught a new undergraduate course for non-majors about applications of psychology to everyday life. Roughly each week, we looked at a new topic, I assigned a few research articles and we discussed them in class.\nThis isn‚Äôt a totally original idea, and I‚Äôm especially grateful to Larisa Heiphetz for sharing the syllabus for her similar ‚ÄúFAQs About Life‚Äù course, and to Laurie Santos for making her free online Coursera course on The Science of Well-Being.\nBut in the spirit of paying it forward, I figured I‚Äôd share what I did.\nFirst, here‚Äôs my syllabus with my list of readings."
  },
  {
    "objectID": "blog/posts/2022-03-03-applied-psych-course/appliedpsych.html#science-communication",
    "href": "blog/posts/2022-03-03-applied-psych-course/appliedpsych.html#science-communication",
    "title": "Teaching a seminar-style applied psychology course",
    "section": "Science communication üìù",
    "text": "Science communication üìù\nOne thing I focused on in the course was effective and accurate science writing, communication, and consumption. My thinking was: if students are going to be learning how to apply psychology to their lives, they also ought to learn a bit about the process by which research gets translated for public consumption too.\nSo I had them read media reports about research in addition to the original research articles, and we talked about the strengths and weaknesses of the reports so they could learn how the media sometimes oversimplifies or exaggerates research results.\nFinally, the course ended with them writing their own blog posts summarizing a psychology research article of their choice, written for a general audience, with some kind of practical takeaway message. You can read their posts here."
  },
  {
    "objectID": "blog/posts/2022-03-03-applied-psych-course/appliedpsych.html#what-i-learned",
    "href": "blog/posts/2022-03-03-applied-psych-course/appliedpsych.html#what-i-learned",
    "title": "Teaching a seminar-style applied psychology course",
    "section": "What I learned üí≠",
    "text": "What I learned üí≠\nSometimes, the concepts we talk about in psych courses can feel divorced from everyday concerns. Personally, I loved teaching psychology through the lens of everyday applications.\nI gave students a survey at the end of the quarter with a single question: What was the most important or useful thing you learned in this course? Here are a few of the most thoughtful responses:\n\nThe most important thing I learned in this course was the differences between the various ways to use social media. Normally, when social media comes up in a class, it is entirely bashing it. In this class however, we looked at how to use social media in a way that makes it beneficial, as well as how it can be dangerous only when used under certain conditions.\n\n\nI appreciated the deep and broad discussion on IQ. Intelligence is often both a weighty and a non-agreed-upon idea, so being able to hear a breadth of opinions in one place as well as a clear definition of what psychologists mean when they say IQ was helpful for me to gather my own thoughts on the topic.\n\n\nAlthough it was not one of the weekly subjects, one of the most useful things I can take away from this course is the confidence to read and understand academic papers. I feel less intimidated by the long length and dry language and more confident in my ability to understand the paper and notice its strengths and flaws. And I am more likely to read a study after seeing an article about it outside of class.\n\n\nI think the coolest and most interesting stuff we talked about was the prejudice topic week. Looking at that topic that you hear a lot about in the news from a psychological side was interesting."
  },
  {
    "objectID": "blog/posts/2022-03-03-applied-psych-course/appliedpsych.html#want-more",
    "href": "blog/posts/2022-03-03-applied-psych-course/appliedpsych.html#want-more",
    "title": "Teaching a seminar-style applied psychology course",
    "section": "Want more? üìö",
    "text": "Want more? üìö\nIf you‚Äôre interested in teaching a class like this, feel free to contact me and I‚Äôd be happy to share any of my materials (e.g., assignments, rubrics)."
  },
  {
    "objectID": "blog/posts/2022-06-21-jeopardy-streaks/jeopardystreaks.html",
    "href": "blog/posts/2022-06-21-jeopardy-streaks/jeopardystreaks.html",
    "title": "Are win streaks on Jeopardy becoming more common?",
    "section": "",
    "text": "Amy Schneider on Jeopardy.\nThree of the top five longest winning streaks in Jeopardy history have happened during the show‚Äôs current 38th season. This has led some to wonder whether there has been some kind of paradigm shift in how Jeopardy is played.\nIt‚Äôs possible the answer is yes, but it would be a little surprising for it to happen so suddenly. If something about the game or how people play it were changing, it‚Äôs more likely that you‚Äôd find changing trends over time.\nI collected Jeopardy win data from every episode from J! Archive, an unofficial database of questions and outcomes from the show, to see if I could find any trends.\n(Collecting and organizing this data turned out to be trickier than I expected. I‚Äôll say a bit more about that later, but you can skip ahead if you‚Äôre interested.)\nI‚Äôll get right to the point. Is there any evidence of more streaks over time? Not really.\nFirst, let‚Äôs look at the streaks.\nThis plot starts with Season 20, after a five-win limit was eliminated (shortly after, Ken Jennings began his record-setting run). I‚Äôve highlighted the top five longest streaks. It‚Äôs easy to see why the latest cluster at the far right might raise some eyebrows. But is it the tail end of a trend?\nLet‚Äôs look at the number of ‚Äúlong‚Äù win streaks over time. I defined a long streak as at least five consecutive wins (the limit when the show began).\nFrom the plot, it looks like maybe there has been a slight increase in long streaks in the past ~15 seasons, but it‚Äôs hard to argue there is a recent uptick (barring Season 38).\nWhat about streak length? The plot below shows the distribution of streak lengths for each season.\nThis plot can be a little confusing at first. Each bar includes most of the win streaks for a season. The darkest shade includes 50% of the win streaks, the medium shade includes 80% of the win streaks, and the lightest shade includes 95% of the win streaks.\nThis plot clearly shows that the most recent season is an extreme outlier. Excluding Season 38, it‚Äôs hard to see a clear increasing trend in average streak length, aside from the fact that win streaks got a little longer after the win limit was eliminated.\nJames Holzhauer, one of the show‚Äôs record-holders, doesn‚Äôt think the recent run of streaks is meaningful: ‚ÄúPeople always assume everything is a paradigm shift ‚Ä¶ when it‚Äôs actually fairly normal for results to occasionally cluster.‚Äù I think the data support his interpretation."
  },
  {
    "objectID": "blog/posts/2022-06-21-jeopardy-streaks/jeopardystreaks.html#data",
    "href": "blog/posts/2022-06-21-jeopardy-streaks/jeopardystreaks.html#data",
    "title": "Are win streaks on Jeopardy becoming more common?",
    "section": "The data",
    "text": "The data\nJ! Archive is an amazing resource, but it‚Äôs really designed for human readers to review individual games, not for computers to download massive amounts of data. I ran into a few difficulties with the way the data was formatted in J! Archive. I‚Äôll share a couple of them partly for interest and partly so that if anyone decides to use my data, they will understand it may contain some errors.\n\nContestant names\nEach game page includes full contestant names at the top of the page and ‚Äúnicknames‚Äù (usually first names ‚Äì the names that show up on people‚Äôs screens on Jeopardy) at the bottom of the page with the final scores.\nThis created a challenge because while there is almost certainly only one ‚ÄúJames Holzhauer‚Äù to compete on Jeopardy, there‚Äôs definitely not one ‚ÄúJames‚Äù to ever compete on Jeopardy.\nTo figure out streaks, it was helpful to first assign a unique identifier to each contestant, like their full name. Then I could figure out which James won or lost each game.\nIn most cases, I could just compare the ‚Äúnicknames‚Äù to the full names and look for a match. But some people named James go by Jim, for example, and in those cases, a direct match won‚Äôt work. So I had to manually add a bunch of special cases shown below.\n\nfind_full_contestant_name &lt;- function(contestant_list, nickname) {\n  # Remove all non-alpha characters \n  #(except for hyphen, period, apostrophe, and internal spaces)\n  n &lt;- str_remove_all(nickname, \"[^a-zA-Z\\\\-.' ]\")\n  n &lt;- str_trim(n)\n  \n  # I've hard-coded some nicknames here to take care of hard cases in the data\n  if (n == \"Mike\") {\n    n &lt;- \"(Mike|Michael)\"\n  }\n  else if (n == \"Dave\") {\n    n &lt;- \"(Dave|David)\"\n  }\n  else if (n == \"Tom\") {\n    n &lt;- \"(Tom|Thomas)\"\n  }\n  else if (n == \"Dottie\") {\n    n &lt;- \"(Dottie|Dorothy)\"\n  }\n  else if (n == \"Ernie\") {\n    n &lt;- \"(Ernie|Ernest)\"\n  }\n  else if (n == \"Charlie\") {\n    n &lt;- \"(Charlie|Charles)\"\n  }\n  else if (n == \"Ray\") {\n    n &lt;- \"(Ray|Arthur)\"\n  }\n  else if (n == \"Sandy\") {\n    n &lt;- \"(Sandy|Sandra)\"\n  }\n  else if (n == \"SSGT Paul\") {\n    n &lt;- \"Paul Croshier\"\n  }\n  else if (n == \"DJ\") {\n    n &lt;- \"(DJ|David)\"\n  }\n  else if (n == \"Sparky\") {\n    n &lt;- \"(Sparky|John)\"\n  }\n  else if (n == \"Air Rob\") {\n    n &lt;- \"(Air Rob|Rob)\"\n  }\n  else if (n == \"BobCat\") {\n    n &lt;- \"(BobCat|Bob)\"\n  }\n  else if (n == \"MaryBeth\") {\n    n &lt;- \"(MaryBeth|Mary Beth)\"\n  }\n  else if (n == \"zgn\") {\n    n &lt;- \"Ozgun\"\n  }\n  else if (n == \"Julin\") {\n    n &lt;- \"Juli√°n\"\n  }\n  else if (n == \"Franois\") {\n    n &lt;- \"Francois\"\n  }\n  else if (n == \"Rene\" || n == \"Ren√©e\") {\n    n &lt;- \"(Rene|Ren√©e)\"\n  }\n  else if (n == \"Rb\") {\n    n &lt;- \"R√∏b\"\n  }\n  else if (n == \"Dr. Oz\") {\n    n &lt;- \"Dr. Mehmet Oz\"\n  }\n  else if (n == \"Desire\") {\n    n &lt;- \"(Desir√©e|Desire)\"\n  }\n  else if (n == \"Frdrique\") {\n    n &lt;- \"Fr√©d√©rique\"\n  }\n  else if (n == \"Genevive\") {\n    n &lt;- \"Genevi√®ve\"\n  }\n  else if (n == \"Nio\") {\n    n &lt;- \"Ni√±o\"\n  }\n  else if (n == \"Steve-O\") {\n    n &lt;- \"Steven\"\n  }\n  else if (n == \"Csar\") {\n    n &lt;- \"C√©sar\"\n  }\n  else if (n == \"Sebastin\") {\n    n &lt;- \"Sebasti√°n\"\n  }\n  else if (n == \"Nadge\") {\n    n &lt;- \"Nad√®ge\"\n  }\n  else if (n == \"Andrs\") {\n    n &lt;- \"Andr√©s\"\n  }\n  else if (n == \"Ramn\") {\n    n &lt;- \"Ram√≥n\"\n  }\n  else if (n == \"Anglica\") {\n    n &lt;- \"Ang√©lica\"\n  }\n\n  \n  # Cycle through all names in contestant list and look for a match\n  for (c in contestant_list) {\n    if (!is.na(str_match(str_to_lower(c), str_to_lower(n))[1])) {\n      return(c)\n    }\n  }\n  \n  # No matches found\n  print(paste(contestant_list[1],\n              contestant_list[2],\n              contestant_list[3], n,sep = \",\"))\n  stop(\"Error: find_full_contestant_name found no name matches.\")\n}\n\nA number of the special cases were to match names that contained characters that I had stripped away at the beginning (which I did because I found a contestant whose nickname was in the archive as ‚ÄúJen :)‚Äù)\n\n\nInterrupted streaks\nJeopardy occasionally runs special events like tournaments or college championships that are scheduled in advance and interrupt regular play. When identifying streaks, I had to skip past these events somehow.\nLuckily, J! Archive includes a ‚Äúcomments‚Äù field at the top of every page with notes about each episode like whether it was a tournament. So again, I used a brute-force approach, matching the contents of the comments field to a list in order to filter out all these special events.\nThe difficulty, yet again, was figuring out how many different special events there were (I‚Äôm still not sure I caught them all). Here‚Äôs the snippet of code where I tried to exclude tournaments, championships, kids weeks, celebrity Jeopardy, and more.\n\nstreaks_by_season &lt;- j_data |&gt; \n  filter(is.na(str_match(str_to_lower(comments), \"tournament\"))) |&gt;\n  filter(is.na(str_match(str_to_lower(comments), \"championship\"))) |&gt;\n  filter(is.na(str_match(str_to_lower(comments), \"battle\"))) |&gt;\n  filter(is.na(str_match(str_to_lower(comments), \"all-star\"))) |&gt;\n  filter(is.na(str_match(str_to_lower(comments), \"celebrity\"))) |&gt;\n  filter(is.na(str_match(str_to_lower(comments), \"kids\"))) |&gt;\n  filter(is.na(str_match(str_to_lower(comments), \"week\"))) |&gt;"
  },
  {
    "objectID": "blog/posts/2022-06-21-jeopardy-streaks/jeopardystreaks.html#code",
    "href": "blog/posts/2022-06-21-jeopardy-streaks/jeopardystreaks.html#code",
    "title": "Are win streaks on Jeopardy becoming more common?",
    "section": "Code",
    "text": "Code\nThe full analysis code I used to generate these plots is available here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to Psychology (previous syllabus)\nCognitive Psychology (previous syllabus)\n\n\n\nApplied Psychology (previous syllabus)\n\n\n\nMoral Psychology (previous syllabus)\nComputational Psychology (previous syllabus)"
  },
  {
    "objectID": "teaching/teaching.html#courses",
    "href": "teaching/teaching.html#courses",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to Psychology (previous syllabus)\nCognitive Psychology (previous syllabus)\n\n\n\nPsychology for a Better World (previous syllabus)\n\n\n\nSocial Psychology (previous syllabus)\nMoral Psychology (previous syllabus)"
  },
  {
    "objectID": "teaching/teaching.html#other-courses",
    "href": "teaching/teaching.html#other-courses",
    "title": "Teaching",
    "section": "Other courses",
    "text": "Other courses\nApplied Psychology (previous syllabus)\nComputational Psychology (previous syllabus)"
  },
  {
    "objectID": "teaching/teaching.html#textbooks",
    "href": "teaching/teaching.html#textbooks",
    "title": "Teaching",
    "section": "Textbooks",
    "text": "Textbooks\nIntroduction to Computational Psychology. Online textbook used in my Computational Psychology course."
  },
  {
    "objectID": "starwars/starwars.html",
    "href": "starwars/starwars.html",
    "title": "Star Wars around the world",
    "section": "",
    "text": "Washington, D.C., 1997\n\n\n\n\n\n\n\nSan Francisco, 1998\n\n\n\n\n\n\n\n\n\nFresno, October 1998\n\n\n\n\n\n\n\nLos Angeles, May 2004\n\n\n\n\n\n\n\n\n\nBaltimore, July 2007\n\n\n\n\n\n\n\nDisneyland, November 2007\n\n\n\n\n\n\n\n\n\nBoston, October 2009\n\n\n\n\n\n\n\nPittsburgh, November 2009\n\n\n\n\n\n\n\n\n\nPittsburgh, April 2010\n\n\n\n\n\n\n\nWashington, D.C, April 2011\n\n\n\n\n\n\n\n\n\nPittsburgh, Februrary 2012\n\n\n\n\n\n\n\nIndianapolis, August 2013\n\n\n\n\n\n\n\n\n\nSeattle, July 2014\n\n\n\n\n\n\n\nHollywood, October 2014\n\n\n\n\n\n\n\n\n\nTerre Haute, Indiana, December 2014\n\n\n\n\n\n\n\nDisneyland, December 2014\n\n\n\n\n\n\n\n\n\nMackinac Island, Michigan, June 2015\n\n\n\n\n\n\n\nLondon, July 2015\n\n\n\n\n\n\n\n\n\nFresno, December 2015\n\n\n\n\n\n\n\nSt.¬†Louis, April, 2016\n\n\n\n\n\n\n\n\n\nSeattle, July 2016\n\n\n\n\n\n\n\nNew York City, August 2016\n\n\n\n\n\n\n\n\n\nDisney World, February 2017\n\n\n\n\n\n\n\nFort Wayne, Indiana, April 2017\n\n\n\n\n\n\n\n\n\nSt.¬†Petersburg, Florida, January 2018\n\n\n\n\n\n\n\nDisneyland, June 2018\n\n\n\n\n\n\n\n\n\nMadison, Wisconsin, July 2018\n\n\n\n\n\n\n\nNiagara Falls, October 2018\n\n\n\n\n\n\n\n\n\nChicago, April 2019\n\n\n\n\n\n\n\nIndianapolis, December 2019\n\n\n\n\n\n\n\n\n\nSan Francisco, February 2020\n\n\n\n\n\n\n\nCincinnati, April 2021\n\n\n\n\n\n\n\n\n\nPittsburgh, May 2021\n\n\n\n\n\n\n\nDisneyland, August 2022\n\n\n\n\n\n\n\n\n\nPortland, May 2024\n\n\n\n\n\n\n\nAlbuquerque, June 2025\n\n\n\n\n\n\n\n\n\nDurham, North Carolina, August 2025"
  },
  {
    "objectID": "blog/posts/2022-06-07-r-place/rplace.html",
    "href": "blog/posts/2022-06-07-r-place/rplace.html",
    "title": "An analysis of Reddit‚Äôs r/place",
    "section": "",
    "text": "The final r/place canvas.\nOne of my favorite things on the internet this year was Reddit‚Äôs r/place, a multi-day collaborative art project in which each user could set the color of one pixel every five minutes on a giant 2000x2000 pixel canvas.\nWith millions of users, the result could easily have been a chaotic mess. Instead, subreddit communities coordinated and cooperated to produce something pretty amazing (see the final product at the top of this post).\nAfter r/place ended, Reddit released all the pixel data from the event. I thought it might be interesting to see what I could learn from it."
  },
  {
    "objectID": "blog/posts/2022-06-07-r-place/rplace.html#how-did-the-event-unfold",
    "href": "blog/posts/2022-06-07-r-place/rplace.html#how-did-the-event-unfold",
    "title": "An analysis of Reddit‚Äôs r/place",
    "section": "How did the event unfold?",
    "text": "How did the event unfold?\nOne of the most fascinating parts of watching r/place happen was all the ensuing drama ‚Äì the feuds, the conflicts, the warring factions, the invasions. Groups were constantly battling over territory, there was a purely destructive group known as The Black Void whose goal was just to erase everything with black pixels, and certain areas were frequent targets for vandalism.\nYou can get a sense of all that by watching this timelapse video.\nWhat did the event look like from the standpoint of the raw data? First, let‚Äôs look at pixel placement activity over time.\n\nOverall activity over time üìà\n\n\n\n\n\nThe most obvious trend is increasing activity over time. Interest likely went up as more people learned about r/place.\nThe plot also shows moderator activity (the colored bands). This was a point of controversy and speculation ‚Äì specifically, that admins were censoring stuff they didn‚Äôt like. But according to the data, the moderators took a pretty hands-off approach, only intervening 19 times.\nWhen they did intervene, they drew a colored square over some objectionable content. The bands in the plot show the colors the mods used; the widths of the bands are relative to the size of the squares the mods drew.\n\n\nWhich colors were most popular? üé®\nAt two points in the event, the admins increased the number of available pixel colors. By the end, there were 32 colors to pick from.\nLet‚Äôs look at which colors were most popular and when.\n\n\n\n\n\nBlack and white were far and away the most popular colors.\nYou can see the two points when the new sets of colors were introduced. Most of the new colors never overtook the original colors in popularity, but a couple did.\nKeep in mind that these totals don‚Äôt necessarily represent how prevalent a color is in the final canvas. This plot just shows how often people placed pixels with each color. That color may have been overwritten numerous times.\n\n\nWhich parts of the canvas were most contentious? ü•ä\nWe can get the best visual sense of the drama that unfolded on r/place by looking at which pixels had the most activity. If a pixel was overwritten many times, that‚Äôs a sign that users were fighting over that area of the canvas.\nIn the picture below, I‚Äôve made a heatmap where brighter spots indicate areas where there was more activity and darker spots indicate areas with less activity.\n\n\n\n\n\nThe amazing thing about this image is that, if you look carefully, you can make out many parts of the final canvas, like the Turkish flag or the osu! logo. This shows how contentious these areas were: they were frequently under attack and lots of work was put into preserving them.\nThe most active pixel was changed nearly 100,000 times. It happened to be the most upper-left pixel in the canvas. On the other end of the spectrum, there were nearly 2900 pixels (0.07% of the canvas) that were placed only once for the entire duration of the event. No pixels were completely untouched."
  },
  {
    "objectID": "blog/posts/2022-06-07-r-place/rplace.html#individual-user-activity",
    "href": "blog/posts/2022-06-07-r-place/rplace.html#individual-user-activity",
    "title": "An analysis of Reddit‚Äôs r/place",
    "section": "Individual user activity üíª",
    "text": "Individual user activity üíª\nOne specific question I had was whether most activity was driven by a small fraction of users. This is a common phenomenon online where, for example, a small percentage of users are responsible for the vast majority of comments. This is actually a broader phenomenon known as Zipf‚Äôs law in which, essentially, the most common items in a set are vastly more common than the the less common items. Was that true here? For example, I found that over 2.3 million users in this dataset only placed a single pixel.\nI pulled the top 100,000 most prolific users (i.e., users that placed the most pixels) and plotted the number of pixels they placed in order of their their rank.\n\n\n\n\n\nThe results don‚Äôt quite follow Zipf‚Äôs law. It turns out the most extremely prolific users (who placed 600+ pixels) weren‚Äôt that much more prolific than thousands of others who placed 200+ pixels.\nLet‚Äôs instead look at what they actually did, starting with the most prolific user, who placed 795 pixels. Below, I‚Äôve traced this user‚Äôs path over roughly 3.5 days.\n\n\n\n\n\nLooking at this user‚Äôs most frequent contributions, and after doing a bit of research on the r/place atlas I inferred that they are a My Little Pony fan. The My Little Pony community apparently started working in the upper left part of the canvas but moved to the upper right and later to the bottom, due to conflict with followers of a Twitch streamer. ü§∑\nThis extremely active brony moved around a fair amount. Let‚Äôs look at some other users who barely moved at all. In the figure below, I‚Äôve traced the paths of 10 of the least mobile users among the top 1000 most prolific.\n\n\n\n\n\nThe colors here are just meant to distinguish between users.\nAmong these users was one (the upper-leftmost one) who placed 539 pixels in roughly 58 hours. 443 of those pixels were placed in just a single location. That location? The upper left in the ‚Äúconnection lost‚Äù area (some kind of long-running inside joke). The color? Orange-red. That area pretty much never changed (and was always black). So this person basically accomplished nothing and was almost certainly a bot (more on that in a bit).\nAnother user in the figure above (the lime green colored one in the upper left) placed 501 pixels in a roughly a 50-hour period, all within a 36x41 pixel area. That area? The Canadian flag. The hilarious tragedy of the Canadian flag is documented here. What is impossible to determine is whether this person was trying to vandalize the flag or repair the vandalism."
  },
  {
    "objectID": "blog/posts/2022-06-07-r-place/rplace.html#the-rise-of-the-bots",
    "href": "blog/posts/2022-06-07-r-place/rplace.html#the-rise-of-the-bots",
    "title": "An analysis of Reddit‚Äôs r/place",
    "section": "The rise of the bots ü§ñ",
    "text": "The rise of the bots ü§ñ\nAnother major controversy was over the use of ‚Äúbots‚Äù ‚Äì scripts that automatically placed pixels for you whenever your five-minute cool-down period was up. They weren‚Äôt forbidden, but some people saw them as a kind of cheating.\nAccusations of widespread bot use were common. But can we find evidence for that in the data?\nI looked at every user who placed at least 20 pixels. I then looked at the percentage of times they placed a pixel within 303 seconds (5 minutes + 3 seconds) of their last pixel. In other words, how often were they placing pixels almost as soon as they possibly could?\nHere are the results.\n\n\n\n\n\nIt looks like bots were relatively rare. And contrary to my expectations, even among the users that placed hundreds of pixels, they weren‚Äôt generally more likely to be bots. Some people who placed 400+ pixels almost never placed them immediately after their 5 minute cool-off periods, suggesting they were placing them manually.\nI chose 3 seconds after the cool-off period ended somewhat arbitrarily, so I re-ran the analysis varying this delay from 0 to 10 seconds. Then I computed what percentage of pixels overall were placed within that period of time.\n\n\n\n\n\nObviously, the longer the delay, the more pixels get included. But even for delays up to 10 seconds, no more than 15% of all pixels were placed.\nThe bottom line: Surely some people were using scripts. Perhaps as many as 5-10% of pixels were placed by scripts. But this analysis suggests that the vast majority of users weren‚Äôt using them and the vast majority of pixels were manually placed."
  },
  {
    "objectID": "blog/posts/2022-06-07-r-place/rplace.html#code",
    "href": "blog/posts/2022-06-07-r-place/rplace.html#code",
    "title": "An analysis of Reddit‚Äôs r/place",
    "section": "Code",
    "text": "Code\nThe full analysis code I used to generate these plots is available here."
  },
  {
    "objectID": "teaching/teaching.html#tentative-2024-2025-courses",
    "href": "teaching/teaching.html#tentative-2024-2025-courses",
    "title": "Teaching",
    "section": "Tentative 2024-2025 courses",
    "text": "Tentative 2024-2025 courses\n\nFall\nIntroduction to Psychology\nCognitive Psychology\n\n\nWinter\nTBD (new course)\n\n\nSpring\nSocial Psychology (previous syllabus)\nMoral Psychology (previous syllabus)"
  },
  {
    "objectID": "teaching/teaching.html#fall",
    "href": "teaching/teaching.html#fall",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to Psychology (previous syllabus)\nCognitive Psychology (previous syllabus)"
  },
  {
    "objectID": "teaching/teaching.html#winter",
    "href": "teaching/teaching.html#winter",
    "title": "Teaching",
    "section": "",
    "text": "Applied Psychology (previous syllabus)"
  },
  {
    "objectID": "teaching/teaching.html#spring",
    "href": "teaching/teaching.html#spring",
    "title": "Teaching",
    "section": "",
    "text": "Moral Psychology (previous syllabus)\nComputational Psychology (previous syllabus)"
  },
  {
    "objectID": "teaching/teaching.html#fall-1",
    "href": "teaching/teaching.html#fall-1",
    "title": "Teaching",
    "section": "Fall",
    "text": "Fall\nIntroduction to Psychology\nCognitive Psychology"
  },
  {
    "objectID": "teaching/teaching.html#winter-1",
    "href": "teaching/teaching.html#winter-1",
    "title": "Teaching",
    "section": "Winter",
    "text": "Winter\nPsychology for a Better World (previous syllabus)"
  },
  {
    "objectID": "teaching/teaching.html#spring-1",
    "href": "teaching/teaching.html#spring-1",
    "title": "Teaching",
    "section": "Spring",
    "text": "Spring\nIntroduction to Psychology\nSocial Psychology (previous syllabus)"
  }
]