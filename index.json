[{"authors":null,"categories":null,"content":"I\u0026rsquo;m a cognitive scientist and psychologist. I use computational models and behavioral experiments to study how people think and reason. I\u0026rsquo;m primarily interested in social cognition: how people think about other people. I have also studied how people learn and use concepts, and how people revise their beliefs after seeing new evidence.\nI write about TV and psychology at Overthinking TV.\n","date":1646265600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646265600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://alanjern.github.io/author/alan-jern/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alan-jern/","section":"authors","summary":"I\u0026rsquo;m a cognitive scientist and psychologist. I use computational models and behavioral experiments to study how people think and reason. I\u0026rsquo;m primarily interested in social cognition: how people think about other people.","tags":null,"title":"Alan Jern","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Alan Jern FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://alanjern.github.io/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://alanjern.github.io/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://alanjern.github.io/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://alanjern.github.io/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://alanjern.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":["Data analysis"],"content":" Three of the top five longest winning streaks in Jeopardy history have happened during the show‚Äôs current 38th season. This has led some to wonder whether there has been some kind of paradigm shift in how Jeopardy is played.\nIt‚Äôs possible the answer is yes, but it would be a little surprising for it to happen so suddenly. If something about the game or how people play it were changing, it‚Äôs more likely that you‚Äôd find changing trends over time.\nI collected Jeopardy win data from every episode from J! Archive, an unofficial database of questions and outcomes from the show, to see if I could find any trends.\n(Collecting and organizing this data turned out to be trickier than I expected. I‚Äôll say a bit more about that later, but you can skip ahead if you‚Äôre interested.)\nI‚Äôll get right to the point. Is there any evidence of more streaks over time? Not really.\nFirst, let‚Äôs look at the streaks.\nThis plot starts with Season 20, after a five-win limit was eliminated (shortly after, Ken Jennings began his record-setting run). I‚Äôve highlighted the top five longest streaks. It‚Äôs easy to see why the latest cluster at the far right might raise some eyebrows. But is it the tail end of a trend?\nLet‚Äôs look at the number of ‚Äúlong‚Äù win streaks over time. I defined a long streak as at least five consecutive wins (the limit when the show began).\nFrom the plot, it looks like maybe there has been a slight increase in long streaks in the past ~15 seasons, but it‚Äôs hard to argue there is a recent uptick (barring Season 38).\nWhat about streak length? The plot below shows the distribution of streak lengths for each season.\nThis plot can be a little confusing at first. Each bar includes most of the win streaks for a season. The darkest shade includes 50% of the win streaks, the medium shade includes 80% of the win streaks, and the lightest shade includes 95% of the win streaks.\nThis plot clearly shows that the most recent season is an extreme outlier. Excluding Season 38, it‚Äôs hard to see a clear increasing trend in average streak length, aside from the fact that win streaks got a little longer after the win limit was eliminated.\nJames Holzhauer, one of the show‚Äôs record-holders, doesn‚Äôt think the recent run of streaks is meaningful: ‚ÄúPeople always assume everything is a paradigm shift ‚Ä¶ when it‚Äôs actually fairly normal for results to occasionally cluster.‚Äù I think the data support his interpretation.\nThe data J! Archive is an amazing resource, but it‚Äôs really designed for human readers to review individual games, not for computers to download massive amounts of data. I ran into a few difficulties with the way the data was formatted in J! Archive. I‚Äôll share a couple of them partly for interest and partly so that if anyone decides to use my data, they will understand it may contain some errors.\nContestant names Each game page includes full contestant names at the top of the page and ‚Äúnicknames‚Äù (usually first names ‚Äì the names that show up on people‚Äôs screens on Jeopardy) at the bottom of the page with the final scores.\nThis created a challenge because while there is almost certainly only one ‚ÄúJames Holzhauer‚Äù to compete on Jeopardy, there‚Äôs definitely not one ‚ÄúJames‚Äù to ever compete on Jeopardy.\nTo figure out streaks, it was helpful to first assign a unique identifier to each contestant, like their full name. Then I could figure out which James won or lost each game.\nIn most cases, I could just compare the ‚Äúnicknames‚Äù to the full names and look for a match. But some people named James go by Jim, for example, and in those cases, a direct match won‚Äôt work. So I had to manually add a bunch of special cases shown below.\nfind_full_contestant_name \u0026lt;- function(contestant_list, nickname) { # Remove all non-alpha characters #(except for hyphen, period, apostrophe, and internal spaces) n \u0026lt;- str_remove_all(nickname, \u0026quot;[^a-zA-Z\\\\-.\u0026#39; ]\u0026quot;) n \u0026lt;- str_trim(n) # I\u0026#39;ve hard-coded some nicknames here to take care of hard cases in the data if (n == \u0026quot;Mike\u0026quot;) { n \u0026lt;- \u0026quot;(Mike|Michael)\u0026quot; } else if (n == \u0026quot;Dave\u0026quot;) { n \u0026lt;- \u0026quot;(Dave|David)\u0026quot; } else if (n == \u0026quot;Tom\u0026quot;) { n \u0026lt;- \u0026quot;(Tom|Thomas)\u0026quot; } else if (n == \u0026quot;Dottie\u0026quot;) { n \u0026lt;- \u0026quot;(Dottie|Dorothy)\u0026quot; } else if (n == \u0026quot;Ernie\u0026quot;) { n \u0026lt;- \u0026quot;(Ernie|Ernest)\u0026quot; } else if (n == \u0026quot;Charlie\u0026quot;) { n \u0026lt;- \u0026quot;(Charlie|Charles)\u0026quot; } else if (n == \u0026quot;Ray\u0026quot;) { n \u0026lt;- \u0026quot;(Ray|Arthur)\u0026quot; } else if (n == \u0026quot;Sandy\u0026quot;) { n \u0026lt;- \u0026quot;(Sandy|Sandra)\u0026quot; } else if (n == \u0026quot;SSGT Paul\u0026quot;) { n \u0026lt;- \u0026quot;Paul Croshier\u0026quot; } else if (n == \u0026quot;DJ\u0026quot;) { n \u0026lt;- \u0026quot;(DJ|David)\u0026quot; } else if (n == \u0026quot;Sparky\u0026quot;) { n \u0026lt;- \u0026quot;(Sparky|John)\u0026quot; } else if (n == \u0026quot;Air Rob\u0026quot;) { n \u0026lt;- \u0026quot;(Air Rob|Rob)\u0026quot; } else if (n == \u0026quot;BobCat\u0026quot;) { n \u0026lt;- \u0026quot;(BobCat|Bob)\u0026quot; } else if (n == \u0026quot;MaryBeth\u0026quot;) { n \u0026lt;- \u0026quot;(MaryBeth|Mary Beth)\u0026quot; } else if (n == \u0026quot;zgn\u0026quot;) { n \u0026lt;- \u0026quot;Ozgun\u0026quot; } else if (n == \u0026quot;Julin\u0026quot;) { n \u0026lt;- \u0026quot;Juli√°n\u0026quot; } else if (n == \u0026quot;Franois\u0026quot;) { n \u0026lt;- \u0026quot;Francois\u0026quot; } else if (n == \u0026quot;Rene\u0026quot; || n == \u0026quot;Ren√©e\u0026quot;) { n \u0026lt;- \u0026quot;(Rene|Ren√©e)\u0026quot; } else if (n == \u0026quot;Rb\u0026quot;) { n \u0026lt;- \u0026quot;R√∏b\u0026quot; } else if (n == \u0026quot;Dr. Oz\u0026quot;) { n \u0026lt;- \u0026quot;Dr. Mehmet Oz\u0026quot; } else if (n == \u0026quot;Desire\u0026quot;) { n \u0026lt;- \u0026quot;(Desir√©e|Desire)\u0026quot; } else if (n == \u0026quot;Frdrique\u0026quot;) { n \u0026lt;- \u0026quot;Fr√©d√©rique\u0026quot; } else if (n == \u0026quot;Genevive\u0026quot;) { n \u0026lt;- \u0026quot;Genevi√®ve\u0026quot; } else if (n == \u0026quot;Nio\u0026quot;) { n \u0026lt;- \u0026quot;Ni√±o\u0026quot; } else if (n == \u0026quot;Steve-O\u0026quot;) { n \u0026lt;- \u0026quot;Steven\u0026quot; } else if (n == \u0026quot;Csar\u0026quot;) { n \u0026lt;- \u0026quot;C√©sar\u0026quot; } else if (n == \u0026quot;Sebastin\u0026quot;) { n \u0026lt;- \u0026quot;Sebasti√°n\u0026quot; } else if (n == \u0026quot;Nadge\u0026quot;) { n \u0026lt;- \u0026quot;Nad√®ge\u0026quot; } else if (n == \u0026quot;Andrs\u0026quot;) { n \u0026lt;- \u0026quot;Andr√©s\u0026quot; } else if (n == \u0026quot;Ramn\u0026quot;) { n \u0026lt;- \u0026quot;Ram√≥n\u0026quot; } else if (n == \u0026quot;Anglica\u0026quot;) { n \u0026lt;- \u0026quot;Ang√©lica\u0026quot; } # Cycle through all names in contestant list and look for a match for (c in contestant_list) { if (!is.na(str_match(str_to_lower(c), str_to_lower(n))[1])) { return(c) } } # No matches found print(paste(contestant_list[1], contestant_list[2], contestant_list[3], n,sep = \u0026quot;,\u0026quot;)) stop(\u0026quot;Error: find_full_contestant_name found no name matches.\u0026quot;) } A number of the special cases were to match names that contained characters that I had stripped away at the beginning (which I did because I found a contestant whose nickname was in the archive as ‚ÄúJen :)‚Äù)\n Interrupted streaks Jeopardy occasionally runs special events like tournaments or college championships that are scheduled in advance and interrupt regular play. When identifying streaks, I had to skip past these events somehow.\nLuckily, J! Archive includes a ‚Äúcomments‚Äù field at the top of every page with notes about each episode like whether it was a tournament. So again, I used a brute-force approach, matching the contents of the comments field to a list in order to filter out all these special events.\nThe difficulty, yet again, was figuring out how many different special events there were (I‚Äôm still not sure I caught them all). Here‚Äôs the snippet of code where I tried to exclude tournaments, championships, kids weeks, celebrity Jeopardy, and more.\nstreaks_by_season \u0026lt;- j_data |\u0026gt; filter(is.na(str_match(str_to_lower(comments), \u0026quot;tournament\u0026quot;))) |\u0026gt; filter(is.na(str_match(str_to_lower(comments), \u0026quot;championship\u0026quot;))) |\u0026gt; filter(is.na(str_match(str_to_lower(comments), \u0026quot;battle\u0026quot;))) |\u0026gt; filter(is.na(str_match(str_to_lower(comments), \u0026quot;all-star\u0026quot;))) |\u0026gt; filter(is.na(str_match(str_to_lower(comments), \u0026quot;celebrity\u0026quot;))) |\u0026gt; filter(is.na(str_match(str_to_lower(comments), \u0026quot;kids\u0026quot;))) |\u0026gt; filter(is.na(str_match(str_to_lower(comments), \u0026quot;week\u0026quot;))) |\u0026gt;   Code The full analysis code I used to generate these plots is available here.\n ","date":1655769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655769600,"objectID":"14cc6cc0d1e66d04ae63d1d9aa5b4a22","permalink":"https://alanjern.github.io/post/are-win-streaks-on-jeopardy-becoming-more-common/","publishdate":"2022-06-21T00:00:00Z","relpermalink":"/post/are-win-streaks-on-jeopardy-becoming-more-common/","section":"post","summary":"A recent unprecedented run of winning streaks on *Jeopardy* has led some to wonder whether something about the show or the contestants has changed. But is there anything in the data to suggest that this is an actual trend? Looking at data from all 38 seasons points to the recent run of streaks more likely being an outlier than part of a trend.","tags":["Jeopardy","R","data visualization"],"title":"Are win streaks on Jeopardy becoming more common?","type":"post"},{"authors":null,"categories":["Data analysis"],"content":" One of my favorite things on the internet this year was Reddit‚Äôs r/place, a multi-day collaborative art project in which each user could set the color of one pixel every five minutes on a giant 2000x2000 pixel canvas.\nWith millions of users, the result could easily have been a chaotic mess. Instead, subreddit communities coordinated and cooperated to produce something pretty amazing (see the final product at the top of this post).\nAfter r/place ended, Reddit released all the pixel data from the event. I thought it might be interesting to see what I could learn from it.\nHow did the event unfold? One of the most fascinating parts of watching r/place happen was all the ensuing drama ‚Äì the feuds, the conflicts, the warring factions, the invasions. Groups were constantly battling over territory, there was a purely destructive group known as The Black Void whose goal was just to erase everything with black pixels, and certain areas were frequent targets for vandalism.\nYou can get a sense of all that by watching this timelapse video.\nWhat did the event look like from the standpoint of the raw data? First, let‚Äôs look at pixel placement activity over time.\nOverall activity over time üìà The most obvious trend is increasing activity over time. Interest likely went up as more people learned about r/place.\nThe plot also shows moderator activity (the colored bands). This was a point of controversy and speculation ‚Äì specifically, that admins were censoring stuff they didn‚Äôt like. But according to the data, the moderators took a pretty hands-off approach, only intervening 19 times.\nWhen they did intervene, they drew a colored square over some objectionable content. The bands in the plot show the colors the mods used; the widths of the bands are relative to the size of the squares the mods drew.\n Which colors were most popular? üé® At two points in the event, the admins increased the number of available pixel colors. By the end, there were 32 colors to pick from.\nLet‚Äôs look at which colors were most popular and when.\nBlack and white were far and away the most popular colors.\nYou can see the two points when the new sets of colors were introduced. Most of the new colors never overtook the original colors in popularity, but a couple did.\nKeep in mind that these totals don‚Äôt necessarily represent how prevalent a color is in the final canvas. This plot just shows how often people placed pixels with each color. That color may have been overwritten numerous times.\n Which parts of the canvas were most contentious? ü•ä We can get the best visual sense of the drama that unfolded on r/place by looking at which pixels had the most activity. If a pixel was overwritten many times, that‚Äôs a sign that users were fighting over that area of the canvas.\nIn the picture below, I‚Äôve made a heatmap where brighter spots indicate areas where there was more activity and darker spots indicate areas with less activity.\nThe amazing thing about this image is that, if you look carefully, you can make out many parts of the final canvas, like the Turkish flag or the osu! logo. This shows how contentious these areas were: they were frequently under attack and lots of work was put into preserving them.\nThe most active pixel was changed nearly 100,000 times. It happened to be the most upper-left pixel in the canvas. On the other end of the spectrum, there were nearly 2900 pixels (0.07% of the canvas) that were placed only once for the entire duration of the event. No pixels were completely untouched.\n  Individual user activity üíª One specific question I had was whether most activity was driven by a small fraction of users. This is a common phenomenon online where, for example, a small percentage of users are responsible for the vast majority of comments. This is actually a broader phenomenon known as Zipf‚Äôs law in which, essentially, the most common items in a set are vastly more common than the the less common items. Was that true here? For example, I found that over 2.3 million users in this dataset only placed a single pixel.\nI pulled the top 100,000 most prolific users (i.e., users that placed the most pixels) and plotted the number of pixels they placed in order of their their rank.\nThe results don‚Äôt quite follow Zipf‚Äôs law. It turns out the most extremely prolific users (who placed 600+ pixels) weren‚Äôt that much more prolific than thousands of others who placed 200+ pixels.\nLet‚Äôs instead look at what they actually did, starting with the most prolific user, who placed 795 pixels. Below, I‚Äôve traced this user‚Äôs path over roughly 3.5 days.\nLooking at this user‚Äôs most frequent contributions, and after doing a bit of research on the r/place atlas I inferred that they are a My Little Pony fan. The My Little Pony community apparently started working in the upper left part of the canvas but moved to the upper right and later to the bottom, due to conflict with followers of a Twitch streamer. ü§∑\nThis extremely active brony moved around a fair amount. Let‚Äôs look at some other users who barely moved at all. In the figure below, I‚Äôve traced the paths of 10 of the least mobile users among the top 1000 most prolific.\nThe colors here are just meant to distinguish between users.\nAmong these users was one (the upper-leftmost one) who placed 539 pixels in roughly 58 hours. 443 of those pixels were placed in just a single location. That location? The upper left in the ‚Äúconnection lost‚Äù area (some kind of long-running inside joke). The color? Orange-red. That area pretty much never changed (and was always black). So this person basically accomplished nothing and was almost certainly a bot (more on that in a bit).\nAnother user in the figure above (the lime green colored one in the upper left) placed 501 pixels in a roughly a 50-hour period, all within a 36x41 pixel area. That area? The Canadian flag. The hilarious tragedy of the Canadian flag is documented here. What is impossible to determine is whether this person was trying to vandalize the flag or repair the vandalism.\n The rise of the bots ü§ñ Another major controversy was over the use of ‚Äúbots‚Äù ‚Äì scripts that automatically placed pixels for you whenever your five-minute cool-down period was up. They weren‚Äôt forbidden, but some people saw them as a kind of cheating.\nAccusations of widespread bot use were common. But can we find evidence for that in the data?\nI looked at every user who placed at least 20 pixels. I then looked at the percentage of times they placed a pixel within 303 seconds (5 minutes + 3 seconds) of their last pixel. In other words, how often were they placing pixels almost as soon as they possibly could?\nHere are the results.\nIt looks like bots were relatively rare. And contrary to my expectations, even among the users that placed hundreds of pixels, they weren‚Äôt generally more likely to be bots. Some people who placed 400+ pixels almost never placed them immediately after their 5 minute cool-off periods, suggesting they were placing them manually.\nI chose 3 seconds after the cool-off period ended somewhat arbitrarily, so I re-ran the analysis varying this delay from 0 to 10 seconds. Then I computed what percentage of pixels overall were placed within that period of time.\nObviously, the longer the delay, the more pixels get included. But even for delays up to 10 seconds, no more than 15% of all pixels were placed.\nThe bottom line: Surely some people were using scripts. Perhaps as many as 5-10% of pixels were placed by scripts. But this analysis suggests that the vast majority of users weren‚Äôt using them and the vast majority of pixels were manually placed.\n Code The full analysis code I used to generate these plots is available here.\n ","date":1654560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654560000,"objectID":"d5ebf98f96a5aedc07fecf84d30d4acd","permalink":"https://alanjern.github.io/post/an-analysis-of-reddits-r/place/","publishdate":"2022-06-07T00:00:00Z","relpermalink":"/post/an-analysis-of-reddits-r/place/","section":"post","summary":"My favorite thing on the Internet this year was Reddit's r/place, a collaborative art project that lasted several days. Analyzing the official data from the event released by Reddit, I found that, contrary to my expectations, activity wasn't dominated by a small fraction of users and bots were relatively rare.","tags":["Reddit","r/place","R","data visualization"],"title":"An analysis of Reddit's r/place","type":"post"},{"authors":["S. Payne","A. Jern"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"e06d188b0995008501d9f0eeaa21b9e3","permalink":"https://alanjern.github.io/publication/payne-jern-22/","publishdate":"2022-05-01T21:40:27.866452Z","relpermalink":"/publication/payne-jern-22/","section":"publication","summary":"","tags":null,"title":"Do people use social information to improve predictions about everyday events?","type":"publication"},{"authors":["Alan Jern"],"categories":["Teaching"],"content":"This past quarter, I taught a new undergraduate course for non-majors about applications of psychology to everyday life. Roughly each week, we looked at a new topic, I assigned a few research articles and we discussed them in class.\nThis isn\u0026rsquo;t a totally original idea, and I\u0026rsquo;m especially grateful to Larisa Heiphetz for sharing the syllabus for her similar \u0026ldquo;FAQs About Life\u0026rdquo; course, and to Laurie Santos for making her free online Coursera course on The Science of Well-Being.\nBut in the spirit of paying it forward, I figured I\u0026rsquo;d share what I did.\nFirst, here\u0026rsquo;s my syllabus with my list of readings.\nScience communication üìù One thing I focused on in the course was effective and accurate science writing, communication, and consumption. My thinking was: if students are going to be learning how to apply psychology to their lives, they also ought to learn a bit about the process by which research gets translated for public consumption too.\nSo I had them read media reports about research in addition to the original research articles, and we talked about the strengths and weaknesses of the reports so they could learn how the media sometimes oversimplifies or exaggerates research results.\nFinally, the course ended with them writing their own blog posts summarizing a psychology research article of their choice, written for a general audience, with some kind of practical takeaway message. You can read their posts here.\nWhat I learned üí≠ Sometimes, the concepts we talk about in psych courses can feel divorced from everyday concerns. Personally, I loved teaching psychology through the lens of everyday applications.\nI gave students a survey at the end of the quarter with a single question: What was the most important or useful thing you learned in this course? Here are a few of the most thoughtful responses:\n The most important thing I learned in this course was the differences between the various ways to use social media. Normally, when social media comes up in a class, it is entirely bashing it. In this class however, we looked at how to use social media in a way that makes it beneficial, as well as how it can be dangerous only when used under certain conditions.\n  I appreciated the deep and broad discussion on IQ. Intelligence is often both a weighty and a non-agreed-upon idea, so being able to hear a breadth of opinions in one place as well as a clear definition of what psychologists mean when they say IQ was helpful for me to gather my own thoughts on the topic.\n  Although it was not one of the weekly subjects, one of the most useful things I can take away from this course is the confidence to read and understand academic papers. I feel less intimidated by the long length and dry language and more confident in my ability to understand the paper and notice its strengths and flaws. And I am more likely to read a study after seeing an article about it outside of class.\n  I think the coolest and most interesting stuff we talked about was the prejudice topic week. Looking at that topic that you hear a lot about in the news from a psychological side was interesting.\n Want more? üìö If you\u0026rsquo;re interested in teaching a class like this, feel free to contact me and I\u0026rsquo;d be happy to share any of my materials (e.g., assignments, rubrics).\n","date":1646265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646265600,"objectID":"d47ce1e7dcad078f2d9e1cbeb6111da2","permalink":"https://alanjern.github.io/post/teaching-a-seminar-style-applied-psychology-course/","publishdate":"2022-03-03T00:00:00Z","relpermalink":"/post/teaching-a-seminar-style-applied-psychology-course/","section":"post","summary":"This past quarter, I taught a new undergraduate course for non-majors about applications of psychology to everyday life. Roughly each week, we looked at a new topic, I assigned a few research articles and we discussed them in class.","tags":["applied psychology","teaching","science communication"],"title":"Teaching a seminar-style applied psychology course","type":"post"},{"authors":null,"categories":null,"content":"","date":1646179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646179200,"objectID":"ef6fed5f9add40022458305196fd41ef","permalink":"https://alanjern.github.io/popular/nautilus22/","publishdate":"2022-03-02T00:00:00Z","relpermalink":"/popular/nautilus22/","section":"popular","summary":"Nautilus. March 2, 2022.","tags":null,"title":"Deepfake Luke Skywalker Should Scare Us","type":"popular"},{"authors":null,"categories":["Data analysis"],"content":"  I love Disney World. I hate waiting in line. Sadly you can‚Äôt go to Disney World without waiting in line. But if you plan ahead, you can keep the waiting to a minimum.\nPart of that planning is going at the right time. Visitors are somewhat predictable: they don‚Äôt visit uniformly throughout the year. If you have the flexibility, you can minimize your wait times by going when other people aren‚Äôt. But when are those times?\nTo try to answer that question, I found a great dataset from Touring Plans of posted wait times for several attractions in each park every few minutes of each day going back to 2015.\nA year at Magic Kingdom in waits For Magic Kingdom, Touring Plans has data for three attractions: Pirates of the Caribbean, Seven Dwarfs Mine Train, and Splash Mountain. To get a sense of how waits at Magic Kingdom vary over the calender year, I took waits from every day between January 1st, 2015 and March 15, 2020 (the last day Disney World was open before it shut down for Covid), dropped the year (combining, for example, all waits from January 1st from all years), then computed the mean waits for each day of the year based on the three attractions.\nHere are the results.\nThe plot also shows US holidays (some holidays fell on different days between 2015 and 2021), and estimates of public school breaks. School breaks vary a lot, so I just went with the dates of the Orange County Public School district in Florida, which includes Orlando. The data on US holidays comes from here.\nThere are a few conclusions that you can draw from this plot:\n Waits spike during holidays. Waits rise in the summer. Waits rise in late March/early April, probably due to Spring break. Waits seem to be lowest in the Fall and early December. On any given day, waits vary a lot. The gray bands show the boundaries between the 25th and 75th percentiles for waits each day for the five years in the data. For example, the mean wait on January 1st is 52 minutes, but 25% of the posted waits on that day for the three attractions were under 20 minutes and 25% of the posted waits were over 75 minutes. New Year‚Äôs Eve is hell at Magic Kingdom.  This plot is already pretty helpful, but it would be nice if we would be a little more precise. That is, can we use a statistical model to say, in quantitative terms, how much better or worse the best and worst times to visit are?\n How much of a difference does the time of year make? I fit a linear regression model that used week of the year and whether the day was within a day of a holiday to predict wait times. (Including whether a day was during a school break didn‚Äôt make much difference in terms of predictions, probably because week of the year already captured most of what ‚Äúbreak time‚Äù would.)\nResults On average, days that are on or near holidays have average waits that are about 6 minutes longer than days that aren‚Äôt.\nThe effects of week are shown below. Because the actual wait times are somewhat meaningless (they are just averages of three attractions), I‚Äôve plotted changes in terms of percent. These changes are relative to Week 16 of the year (around mid-April), which is pretty close to an average week at Disney World.\nThe best weeks to visit in terms of low waits are:\n September through early October, when waits can decrease by 20-30% from baseline. first half of December, when waits can decrease by about 20% from baseline.  The worst weeks to visit are:\n The last two weeks of the year and the first week of the year, when waits can increase by over 20% from baseline. Late March to early April, when waits can increase by about 20% from baseline. Mid-July to early August, when waits can increase by 15-20% from baseline.  And don‚Äôt ever go to Disney World on New Years Eve.\n  How has Covid affected waits? Disney World shut down due to Covid on March 15th, 2020. It reopened on July 11th that same year and has remained open ever since. It changed a lot of its operations after reopening. And, initially, visitors were skittish about returning. So how have wait times changed since reopening?\nTo answer this question, I compared the model‚Äôs predicted wait times to the posted wait times from the Touring Plans dataset, collected from the date of reopening.\nNote that there‚Äôs a lot of missing data in the Fall of 2020.\nThis plot suggests that, since reopening, wait times have generally been a little lower than average, except for maybe in July 2021. However, it looks like waits are beginning to return to their usual patterns as of this past Fall. It‚Äôs probably too soon to say whether it‚Äôs a persisent pattern.\nFor comparison, we can do the same analysis for a different park. I fit another model using the same method with Touring Plans data for the Hollywood Studios park in Disney World. Touring Plans provides wait times for four attractions in Hollywood Studios: Rock ‚Äòn‚Äô Roller Coaster, Alien Swirling Saucers, Slinky Dog Dash, and Toy Story Mania.\nHere‚Äôs the comparable plot.\nThe pattern is basically the same. Generally lower-than-expected waits and an apparent return to normal starting this past Fall.\n Should I book my trip? Not quite yet.\nAs a check, I compared my work to the weekly ranking developed by Dave Shute of yourfirstvisit.net. Dave doesn‚Äôt base his recommendations on statistics and data analysis. Instead, they‚Äôre based on his years of firsthand knowledge and experience. Here are his recommendations for 2021.\nSurprisingly, his recommendations don‚Äôt align with mine much at all. Some of his top-ranked weeks are in the middle of summer. After that, he recommends going in mid-April and early May.\nWhat‚Äôs going on? First, Dave factors in more than crowd levels. He considers things like weather and ride closures. September may be a low-crowd time but it‚Äôs also peak hurricane season. And low-crowd times tend to coincide with suboptimal experiences: Disney is well aware of these seasonal patterns, so they reduce operating hours and schedule attraction refurbishments in the off-season. This means you may get less time in the park and fewer things to do if you go during a less popular time.\nSo if your goal is to have the best overall experience (which Dave‚Äôs advice is targeted toward), choosing the the lowest-crowd time may not actually be best.\nSecond, Dave may have some knowledge not captured in the data. He seems to discount some of the weeks (like in early December) as heavy-crowd times that my analysis predicts will have lower than average waits.\n Okay, but all I care about is short waits. Should I book my trip now? All else being equal, lower waits are better. But is this data giving an accurate picture of wait times? Maybe not.\nAside from the fact that the dataset only includes a few attractions per park, the data are posted wait times, not actual wait times. Disney is notorious for inflating posted wait times. After all, people are happier when they get in a line expecting to wait 30 minutes and only end up waiting 15 than when they get in a line expecting to wait 15 and end up waiting 30.\nAdditionally, ever since Disney parks started posting wait times on their mobile apps, it‚Äôs been strongly suspected that they intentionally manipulate posted wait times to disperse crowds. For example, they might inflate wait times in Tomorrowland to push people toward Frontierland on the other side of the park. (To learn more about this, along with the fascinating history of FastPasses and their influence on wait times, check out this nearly two-hour long video. It‚Äôs really interesting, I promise!)\nSo you probably shouldn‚Äôt take the absolute numbers too seriously. But if waits are consistently inflated year-round, you can probably expect to encounter shorter waits in September than in June, for example, and you can treat this data as an indirect measure of crowds levels.\n  Conclusion There is a clear seasonal pattern to waits (or, perhaps more accurately, crowd levels) at Disney World. Exactly how much impact this will have on the quality of your visit is hard to say because that depends on many more factors.\nBut, for the love of God, don‚Äôt go to Disney World on New Year‚Äôs Eve.\n Code and data Data and the full analysis code I used to generate these plots are available here.\n ","date":1639180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639180800,"objectID":"dad701718be5b9412ed9b460fe4368e9","permalink":"https://alanjern.github.io/post/when-are-waits-lowest-at-disney-world/","publishdate":"2021-12-11T00:00:00Z","relpermalink":"/post/when-are-waits-lowest-at-disney-world/","section":"post","summary":"Using data on posted wait times at Magic Kingdom in Disney World spanning about five years, I looked for seasonal patterns in average wait times. While there is considerable variability, there are clear patterns that you can take advantage of if you are flexible with your visit plans.","tags":["Disney World","Magic Kingdom","Hollywood Studios","theme parks","R","data visualization"],"title":"When are waits lowest at Disney World?","type":"post"},{"authors":null,"categories":["Data analysis"],"content":"  One of the many frustrating things about health care in the US is not having a clue what things are going to cost. But this year, a new federal law went into effect requiring hospitals to publish all of their prices.\nI learned about this after reading this article in the New York Times by Sarah Kliff and Josh Katz. They analyzed the price lists at a number of hospitals and found some huge disparities in prices for identical treatments at the same hospitals and some puzzling discrepancies in insurers‚Äô negotiated prices. Notably, sometimes, the listed price for a treatment negotiated by an insurance company was higher than the listed price for a patient with no insurance at all. For insurance plans with high deductibles, this means you could potentially save money by not using your insurance for some treatments.\nThis got me curious about my own insurance and my own local hospital, so I decided to do a similar analysis.\nThe data üóí Union Hospital, the largest health care provider in my town, makes their price list available here. It includes a line for each treatment or procedure for each different insurer. It‚Äôs admittedly a little hard to make sense of. Here‚Äôs a small sample of the data.\n    Code Description Gross charge Max negotiated charge Min negotiated charge Insurer Charge    110.2 ear, nose, mouth, throat, cranial/facial malignancies 37,706.14 5,687.37 5,687.37 Caresource HIP 5687.37  110.2 ear, nose, mouth, throat, cranial/facial malignancies 37,706.14 5,687.37 5,687.37 Healthy Ind MD Wise 5687.37  110.2 ear, nose, mouth, throat, cranial/facial malignancies 37,706.14 5,687.37 5,687.37 Healthy Indiana Anthem 5687.37  110.2 ear, nose, mouth, throat, cranial/facial malignancies 37,706.14 5,687.37 5,687.37 MHS HIP 5687.37  110.4 ear, nose, mouth, throat, cranial/facial malignancies 50,000.16 13,963.29 13,963.29 Caresource HIP 13963.29  110.4 ear, nose, mouth, throat, cranial/facial malignancies 50,000.16 13,963.29 13,963.29 Healthy Ind MD Wise 13963.29    My interpretation of this is that the final column, Charge, is the result of the negotiation process and is what should be considered the final negotiated ‚Äúprice‚Äù of a procedure. The rest of my analysis is based on that assumption.\n Does my own insurance plan save me money? üíµ My insurance plan is Anthem Blue Access PPO. It‚Äôs part of the Blue Cross Blue Shield association, one of the biggest health insurance companies in the country. Like many people in the US, I get it through my employer and don‚Äôt really have much of a choice in the matter.\nI was first interested in whether Anthem‚Äôs negotiated prices at Union were at least as good as those charged to people with no insurance. As Matt Eyles, chief executive of America‚Äôs Health Insurance Plans says in the NYT article, ‚ÄúInsurers want to make sure they are negotiating the best deals they can for their members, to make sure their products have competitive premiums‚Äù.\nThis plot includes every treatment and procedure in Union‚Äôs price list (except for some extremely expensive ones). Each point is one procedure. Points that fall below the white diagonal line are actually cheaper with no insurance than with my Anthem insurance. Because there are so many points bunched up near the diagonal, it‚Äôs hard to tell that actually 91% of the points are on or above the line, suggesting that Anthem‚Äôs negotiated prices are generally good, relative to having no insurance (they are, unsurprisingly, much higher than Medicare‚Äôs prices).\nBut what are those remaining 9% of procedures? Imaging seems to account for a few. Union Hospital charges Anthem about $350 more for MRIs than it does for patients with no insurance. And it charges about $310 more for CT scans.\nFor some other procedures, prices vary depending on several factors, and sometimes Anthem‚Äôs price is lower, and sometimes it isn‚Äôt. For example, here are all the prices for colonoscopies in the price list.\n    Description Anthem price No insurance price    Colonoscopy through stoma; with removal of tumor(s), polyp(s), or other lesion(s) by hot biopsy forceps 2688.12 1881.68  Colonoscopy, flexible; with removal of foreign body(s) 2593.67 1815.57  Colonoscopy, flexible; with control of bleeding, any method 2526.29 1768.41  Colonoscopy, flexible; with transendoscopic balloon dilation 2785.00 2102.55  Colonoscopy, flexible; with ablation of tumor(s), polyp(s), or other lesion(s) (includes pre- and post-dilation and guide wire passage, when performed) 2071.23 1449.86  Colonoscopy through stoma; with removal of tumor(s), polyp(s), or other lesion(s) by snare technique 2785.00 2184.82  Colonoscopy, flexible; with directed submucosal injection(s), any substance 2785.00 2685.72  Colonoscopy, flexible; with band ligation(s) (eg, hemorrhoids) 3044.17 3466.60  Colonoscopy, flexible; diagnostic, including collection of specimen(s) by brushing or washing, when performed (separate procedure) 2785.00 3631.14  Colonoscopy, flexible; with removal of tumor(s), polyp(s), or other lesion(s) by hot biopsy forceps 2785.00 3642.09  Colonoscopy, flexible; with biopsy, single or multiple 2785.00 3705.97  Colonoscopy, flexible; with removal of tumor(s), polyp(s), or other lesion(s) by snare technique 2785.00 3705.97  Colonoscopy through stoma; diagnostic, including collection of specimen(s) by brushing or washing, when performed (separate procedure) 2785.00 3726.00  Colonoscopy through stoma; with biopsy, single or multiple 2785.00 3758.64    There doesn‚Äôt seem to be much rhyme or reason to when the Anthem prices are lower and when the cash (no insurance) prices are lower, but maybe I‚Äôm missing something. (There are some similar oddities in the pricing for C-sections.)\n How do Union Hospital‚Äôs negotiated prices compare overall? üìà The price list includes prices for nearly 150 different insurance plans. How do the rest of them fare? The plot below replicates the first plot but includes prices from all insurers. I‚Äôve separated out the five largest private insurers ‚Äì Aetna, Cigna, Humana, United, and the Blue Cross Blue Shield Association.\nAcross all procedures, insurers‚Äô negotiated prices don‚Äôt compare very favorably with Union Hospital‚Äôs cash prices for people with no insurance. The fact that ‚Äúnon-major‚Äù insurers‚Äô prices beat the cash prices only 34% of the time is especially surprising because that group includes several Medicare plans, whose prices are consistently very low (and almost certainly beat the cash prices every time).\n What does this mean? ü§® Information like this is potentially important for patients to have. But as the NYT article indicates, price discrepancies and inconsistencies are probably more embarrassing for the insurance companies than for the hospitals because the insurers want to be able to claim that they have negotiated competitive prices.\nHow seriously should we take this information? It‚Äôs probably hard to be sure. The price that gets charged for medical care surely depends on lots of factors. So I personally wouldn‚Äôt consider it a guarantee that because an entry in this list shows that a specific procedure is cheaper with no insurance that I‚Äôd be better off not using my insurance to get it.\nBut this information isn‚Äôt made up either. Hospitals do use these price lists as a starting point (at least) for billing. So I think it‚Äôs reasonable to use these price lists to draw some broadly valid conclusions about the value of your health insurance and the approximate costs of your health care.\n How much does this matter? One last point I haven‚Äôt addressed is that the price charged by the hospital is not the same thing as the price you owe as an individual. The difference between a price tag of $50,000 and $60,000 may be irrelevant to you if you have health insurance with a deductible of $2,000 that pays all costs after that deductible is met. So some of these price discrepancies are problems for insurance companies, not for consumers.\nBut, as you‚Äôll note above, some of the cases where Anthem is beat by the ‚Äúno insurance‚Äù rates are well below common deductibles. So these cost differences can affect how much you are paying üíµ.\nCode and data üìë Code and data for this analysis is available here.\n   ","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"9a9954080b66c2c7e6a04799f4aea3d5","permalink":"https://alanjern.github.io/post/can-you-save-money-at-union-hospital-by-not-using-insurance/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/post/can-you-save-money-at-union-hospital-by-not-using-insurance/","section":"post","summary":"Due to a new federal law, hospitals must publish their price lists. The price list for Union Hospital in Terre Haute shows what many other price lists from hospitals around the country do -- that insurers' negotiated prices sometimes don't make sense.","tags":["hospital","Union Hospital","Terre Haute","R","data visualization","health care","health insurance"],"title":"Can you save money at Union Hospital by not using insurance?","type":"post"},{"authors":null,"categories":["Data analysis"],"content":"  Background üîé This week, Terre Haute Police Department Detective Greg Ferency was shot on the job. This is where I live and, as the local news noted, it was the third police death in the THPD in 10 years. For a city of only around 60,000, this seemed like a lot and led my wife to wonder if it was unusually high. I tried to find out.\nPhoto: WTHI\n The data üìí üóÑ I collected data on police officer deaths from the Officer Down Memorial Page. Their database includes ‚Äúline of duty‚Äù deaths from all causes going back to before 1800. I decided to only collect data from 1980 to today (the most recent death in the database was the death in Terre Haute). So my analysis is only of deaths from the last ~40 years.\nüóÑ Data on US county and city populations came from the US Census. This data includes annual population estimates going back about 10 years. For this analysis, I just decided to use the population numbers from the 2010 census.\n How I combined the data I made a few assumptions to get the data in a workable format. First, I narrowed the set of police deaths down to those that occurred in departments that had Police Department or Sheriff's Department in the name.\nThe police deaths dataset only included department names and states. So to determine the exact location of each department, I assumed each department was named after the city or county where it was located (e.g.¬†Dallas Police Department in the city of Dallas). If a department didn‚Äôt follow this naming convention, it was excluded from analysis.\nFinally, I matched up the city and county names with the names in the census data sets. Sometimes they didn‚Äôt match. For example, Nashville appears in the census data set as Nashville-Davidson metropolitan government.\n Results Cumulative deaths compared to population First, let‚Äôs just compare city/county population to the cumulative number of officer deaths since 1980.\npopVsDeaths \u0026lt;- ggplot() + geom_point(data = filter(deathCountsWithLabels, population \u0026lt;= 75000), mapping = aes(x = log10(population), y = numDeaths, shape = locationType), color = \u0026quot;turquoise3\u0026quot;, alpha = 1/3, show.legend = FALSE) + geom_point(data = filter(deathCountsWithLabels, population \u0026gt; 75000), mapping = aes(x = log10(population), y = numDeaths, shape = locationType), color = \u0026quot;darkslategrey\u0026quot;, alpha = 1/3) + geom_point(data = deathCountsHighlights, mapping = aes(x = log10(population), y = numDeaths, shape = locationType), color = \u0026quot;orangered1\u0026quot;, show.legend = FALSE) + geom_text_repel(data = deathCountsWithLabels, mapping = aes(x = log10(population), y = numDeaths, label = locationLabel), size = 3, max.overlaps = Inf) + scale_x_continuous(labels = c(\u0026quot;100\u0026quot;, \u0026quot;1,000\u0026quot;, \u0026quot;10,000\u0026quot;, \u0026quot;100,000\u0026quot;, \u0026quot;1,000,000\u0026quot;, \u0026quot;10,000,000\u0026quot;)) + labs(x = \u0026quot;Population\u0026quot;, y = \u0026quot;Deaths\u0026quot;, shape = \u0026quot;Location Type\u0026quot;, title = str_c(\u0026quot;Officer Deaths, \u0026quot;, startingYear, \u0026quot;-\u0026quot;, finalYear)) + guides(shape = guide_legend(override.aes = list(alpha = 1, color = \u0026quot;black\u0026quot;))) print(popVsDeaths) ‚ö†Ô∏è One problem with this plot is that many departments showing zero deaths are actually mistakes. This is due to the overly simple department-location matching procedure I used.\nOne example is the Miami-Dade Police Department. This got included because it has the phrase Police Department in it. However, Miami-Dade is a county (a large one, with about 2.5 million people living there) and is identified as Miami-Dade County in my county population dataset. So the department name and county name fail to match because the word ‚Äúcounty‚Äù doesn‚Äôt appear in the name of the department.\nThis actually happened a lot, so you can‚Äôt really trust all the zeros. But surely some of the zeros are accurate. For example, this site reports that 16 out of Indiana‚Äôs 92 counties have had no ‚Äúline of duty‚Äù deaths. To not include departments with no deaths would be a mistake. But given the dataset I had, there wasn‚Äôt a reliable way to identify those departments, and I knew for a fact that many of the zeros I found were simply due to name mismatches. So I decided it was best overall to just remove the zeros entirely.\nHere‚Äôs the same plot with the zeros removed.\npopVsDeaths2 \u0026lt;- ggplot() + geom_point(data = filter(deathCountsWithLabels2, population \u0026lt;= 75000), mapping = aes(x = log10(population), y = numDeaths, shape = locationType), color = \u0026quot;turquoise3\u0026quot;, alpha = 1/3, show.legend = FALSE) + geom_point(data = filter(deathCountsWithLabels2, population \u0026gt; 75000), mapping = aes(x = log10(population), y = numDeaths, shape = locationType), color = \u0026quot;darkslategrey\u0026quot;, alpha = 1/3) + geom_point(data = deathCountsHighlights2, mapping = aes(x = log10(population), y = numDeaths, shape = locationType), color = \u0026quot;orangered1\u0026quot;, show.legend = FALSE) + geom_text_repel(data = deathCountsWithLabels2, mapping = aes(x = log10(population), y = numDeaths, label = locationLabel), size = 3, max.overlaps = Inf) + scale_x_continuous(labels = c(\u0026quot;1,000\u0026quot;, \u0026quot;10,000\u0026quot;, \u0026quot;100,000\u0026quot;, \u0026quot;1,000,000\u0026quot;, \u0026quot;10,000,000\u0026quot;)) + labs(x = \u0026quot;Population\u0026quot;, y = \u0026quot;Deaths\u0026quot;, shape = \u0026quot;Location\u0026quot;, title = str_c(\u0026quot;Officer Deaths, \u0026quot;, startingYear, \u0026quot;-\u0026quot;, finalYear), subtitle = \u0026quot;Locations where at least one death occurred\u0026quot;) + guides(shape = guide_legend(override.aes = list(alpha = 1, color = \u0026quot;black\u0026quot;))) print(popVsDeaths2) Many fewer departments (especially from small cities), but same basic shape. In both plots, I‚Äôve colored small towns like Terre Haute in turquoise and also higlighted a few standout locations to help you orient yourself.\n Scaling by population These plots are a little hard to make sense of because the larger cities have way more deaths which squashes the plot vertically and makes it difficult to draw comparisons. So let‚Äôs try scaling the number of deaths by city/county population.\nNote: Arguably it would make more sense to scale by department size, but this information is harder to come by. I assume department size is correlated with population anyway.\nAlso, from this point on, I‚Äôm going to continue to exclude the zero-death locations for the reasons explained above.\nscaledPopsVsDeaths \u0026lt;- ggplot(data = deathCountsNoZeros) + geom_point(mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths), shape = \u0026quot;circle open\u0026quot;, alpha = 1/3) + geom_point(data = deathCountsHighlights2, mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths), shape = \u0026quot;circle\u0026quot;, color = \u0026quot;coral3\u0026quot;, show.legend = FALSE) + geom_text_repel(data = deathCountsWithLabels2, mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths, label = locationLabel), size = 3, max.overlaps = Inf) + scale_x_continuous(labels = c(\u0026quot;1,000\u0026quot;, \u0026quot;10,000\u0026quot;, \u0026quot;100,000\u0026quot;, \u0026quot;1,000,000\u0026quot;, \u0026quot;10,000,000\u0026quot;)) + labs(x = \u0026quot;Population\u0026quot;, y = \u0026quot;Deaths per 10K\u0026quot;, size = \u0026quot;Total deaths\u0026quot;, title = \u0026quot;Officer deaths per 10,000 in the department\u0026#39;s jurisdiction\u0026quot;, subtitle = str_c(\u0026quot;Cumulative deaths, \u0026quot;, startingYear, \u0026quot;-\u0026quot;, finalYear)) + theme(legend.position = \u0026quot;bottom\u0026quot;) print(scaledPopsVsDeaths) This plot suggests that Terre Haute is above average but maybe not a huge outlier, even among cities of similar population size.\nMore generally, this plot suggests there is not much of a relationship between city size and number of officer deaths, at least for cities above a certain size. The reason it looks like there is a relationship between population and deaths is almost certainly due to there being greater variability for smaller towns/departments. For example, a single death in a small town would have a big effect on this metric.\nSo let‚Äôs exclude small towns (population \u0026lt; 50,000) to get a better understanding.\nscaledPopsVsDeaths_noSmallTowns \u0026lt;- ggplot(data = deathCounts_noSmallTowns) + geom_point(mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths), shape = \u0026quot;circle open\u0026quot;, alpha = 1/3) + geom_point(data = deathCountsHighlights_noSmallTowns, mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths), shape = \u0026quot;circle\u0026quot;, color = \u0026quot;coral3\u0026quot;, show.legend = FALSE) + geom_text_repel(data = deathCountsWithLabels_noSmallTowns, mapping = aes(x = log10(population), y = deathsPer10k, size = numDeaths, label = locationLabel), size = 3) + scale_x_continuous(breaks = c(4,5,6,7), labels = c(\u0026quot;10,000\u0026quot;, \u0026quot;100,000\u0026quot;, \u0026quot;1,000,000\u0026quot;, \u0026quot;10,000,000\u0026quot;)) + labs(x = \u0026quot;Population\u0026quot;, y = \u0026quot;Deaths per 10K\u0026quot;, size = \u0026quot;Total deaths\u0026quot;, title = \u0026quot;Officer deaths per 10,000 people in the department\u0026#39;s jurisdiction\u0026quot;, subtitle = str_c(\u0026quot;Cumulative deaths, \u0026quot;, startingYear, \u0026quot;-\u0026quot;, finalYear)) + theme(legend.position = \u0026quot;bottom\u0026quot;) print(scaledPopsVsDeaths_noSmallTowns) This plot allows us to get a better sense of variability, especially among midsize towns. In this plot, Terre Haute does look a bit more like an outlier. In more quantitative terms, among the locations in the plot above, Terre Haute is in the 99th percentile for officer deaths, scaled by population size. It drops to the 93rd percentile if you include the small towns that were in the previous plot.\n Officer deaths over time One last question I had was whether Terre Haute‚Äôs outlier status was driven by the last 10 years. To get a sense of this, let‚Äôs look at officer deaths over time.\ndeathsOverTime \u0026lt;- ggplot(data = deathCountsByYear) + geom_step(mapping = aes(x = EOW, y = deathsPer10k, group = location_long), alpha = 1/12) + geom_step(data = deathCountsByYearHighlights, mapping = aes(x = EOW, y = deathsPer10k, group = location_long), color = \u0026quot;gold2\u0026quot;, alpha = 2/3) + geom_step(data = filter(deathCountsByYear, location_long == \u0026quot;Terre Haute, IN\u0026quot;), mapping = aes(x = EOW, y = deathsPer10k), color = \u0026quot;coral3\u0026quot;, size = 1.5) + geom_text_repel(data = filter(deathCountsByYearHighlights, EOW == today()), mapping = aes(x = EOW, y = deathsPer10k, group = location_long, label = location_long), size = 3) + labs(title = \u0026quot;Cumulative officer deaths\u0026quot;, subtitle = \u0026quot;Jurisdictions with populations \u0026gt; 50,000\u0026quot;, x = \u0026quot;Date\u0026quot;, y = \u0026quot;Total deaths per 10,000 residents\u0026quot;) + theme_minimal() + theme(panel.grid.minor.x = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank()) print(deathsOverTime) This plot shows cumulative deaths since 1980 for each department serving an area over 50,000 (again limited to only those departments that had at least one death). Terre Haute is highlighted. It gives the impression that Terre Haute wasn‚Äôt much of an outlier until the past 10 years when the last three officers died.\nIt also shows that no officers died in Terre Haute for several decades before 2011.\n  Conclusion ‚òëÔ∏è I think it‚Äôs fair to say that, right now, Terre Haute stands out statistically. The question is whether that‚Äôs indicative of anything meaningful. With thousands of jurisdictions across the US, some are bound to have unusually high numbers of deaths due to random chance alone. So it‚Äôs hard to draw any broad conclusions from these results without more information.\nFor example, speaking for myself, as someone pretty ignorant of law enforcement, I imagined ‚Äúline of duty‚Äù deaths to be the result of attacks at the hands of suspects. But a number of the deaths in the dataset were caused by things like car crashes, Covid-19 (from the past two years), and heart attacks ‚Äì things that happened while on the job but might be hard to attribute to the job itself. My point isn‚Äôt to diminish any of these deaths at all, only to point out that more deaths isn‚Äôt necessarily a sign of some underlying cause or problem. You would need to look at the causes of death (among other factors) before drawing a conclusion like that.\nCode and data üìë Data and the full analysis code I used to generate these plots are available here.\n  ","date":1625788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625788800,"objectID":"ce4c0b6d8b74588c409c064e68f8d462","permalink":"https://alanjern.github.io/post/three-police-offers-have-died-in-terre-haute-in-the-last-10-years.-is-that-a-lot/","publishdate":"2021-07-09T00:00:00Z","relpermalink":"/post/three-police-offers-have-died-in-terre-haute-in-the-last-10-years.-is-that-a-lot/","section":"post","summary":"A THPD detective was shot and killed this week, making it the third officer death in Terre Haute, Indiana in the past 10 years. I compared officer death counts to find out if this is a large number given Terre Haute's relatively small size.","tags":["police","law enforcement","Terre Haute","R","data visualization"],"title":"Three police offers have died in Terre Haute in the last 10 years. Is that a lot?","type":"post"},{"authors":["Alan Jern"],"categories":["Teaching"],"content":"The course üéí It\u0026rsquo;s useful to know a bit about my course first to help you decide if the information in this post will be useful to you.\nWhere I teach doesn\u0026rsquo;t have any psych majors and my course doesn\u0026rsquo;t have any psych pre-reqs. It\u0026rsquo;s a small school that doesn\u0026rsquo;t have a Qualtrics license. The students are mostly science and engineering majors so they are generally tech-savvy, but they don\u0026rsquo;t all have much programming experience.\nThe course itself is focused on replication \u0026ndash; students don\u0026rsquo;t design their own studies \u0026ndash; and I choose which studies they will be replicating. So I have control over the complexity of the study designs they\u0026rsquo;ll be using.\n(You can see the course syllabus here.)\nSo what I was looking for was a platform for running online studies that was free, pretty easy to learn, and had enough base functionality to create some simple study designs.\nWhat is formr? üõ†Ô∏è Enter formr. Compared to alternatives, formr best met all these criteria. Some other options to consider:\n Qualtrics: Most behavioral scientists already know Qualtrics. It\u0026rsquo;s easy to use and super flexible. But it\u0026rsquo;s not free and I don\u0026rsquo;t have access to it where I work. PsyToolkit: I\u0026rsquo;ve used PsyToolkit in the past. It\u0026rsquo;s fairly easy to learn, even for people with little programming experience. It\u0026rsquo;s probably a better choice over formr if you need to do more cognitive-type experiments where you control what people see on the screen at certain times and collect response time data. But I don\u0026rsquo;t really like the interface and it\u0026rsquo;s more advanced than what I needed. jsPsych: Another good choice if you want more advanced options. But it would require teaching more programming than my course had time for.  What worked üëç Despite the somewhat confusing documentation, students caught on to the basics of formr pretty quickly. To supplement the official documentation, I made the following tutorial that you are welcome to share1.\n  Although I didn\u0026rsquo;t get any specific reports of this, another benefit of formr is that you initially build the experiment in Google Sheets, which was nice when students were working in groups because it was so easy for them to collaborate.\nWhat didn\u0026rsquo;t work üôÑ There were some issues with the formr.org website itself. I\u0026rsquo;m incredibly grateful to the formr team, but it\u0026rsquo;s a small operation and it\u0026rsquo;s probably not equipped to handle the rising interest formr has received over the past year or so. I believe this was responsible for the two main problems I encountered:\n  In order to make experiments on the formr.org website, you need an admin account, and new accounts have to be manually approved. After my students signed up, they weren\u0026rsquo;t approved for about 1.5 weeks. In the end, this wasn\u0026rsquo;t catastrophic, but our term is only 10 weeks, and it caused me to have to delay the deadline of the first formr-based assignment. Not ideal.\n  Once students were in the thick of collecting data for their final projects, I got multiple reports of the formr site going down. Also not good.\n  The solution? I learned that the formr team actually recommends installing your own instance of formr so you don\u0026rsquo;t have to rely on their website.\nI haven\u0026rsquo;t done this myself so I can\u0026rsquo;t say anything about it. But my plan for the next time I teach this course is to talk to the IT department at my school and see about installing formr on a college server. This should address the sign-up and downtime problems that we suffered this past term.\ntl;dr üìî formr is free, requires no set-up (if you run it from the existing website) and can be used to do most of what Qualtrics can do with minimal learning. (It can do a lot more advanced stuff too if you are willing to learn more.)\nI think it\u0026rsquo;s a great choice for research methods courses that want to offer students a simple solution for building good-looking ‚ú® and functional ‚öôÔ∏è online studies.\n  I learned after I made this video that the advice I give in it for randomizing subjects equally between conditions is maybe not the optimal method (though it might be the simplest). See here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1623888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623888000,"objectID":"3a1c5d265cfb0852e35f7f7617baa02c","permalink":"https://alanjern.github.io/post/teaching-research-methods-using-formr/","publishdate":"2021-06-17T00:00:00Z","relpermalink":"/post/teaching-research-methods-using-formr/","section":"post","summary":"In the Spring 2021 term, I taught a research methods course in which students learned to build online studies using formr. I explain what worked, what didn't, and why I recommend using formr to others.","tags":["formr","research methods"],"title":"Teaching research methods using formr","type":"post"},{"authors":["A. Jern","A. Derrow-Pinion","AJ Piergiovanni"],"categories":null,"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"a24c8ab1cde34d27f601426738192377","permalink":"https://alanjern.github.io/publication/jern-et-al-21/","publishdate":"2021-06-10T20:29:59.992808Z","relpermalink":"/publication/jern-et-al-21/","section":"publication","summary":"","tags":null,"title":"A computational framework for understanding the roles of simplicity and rational support in people's behavior explanations","type":"publication"},{"authors":null,"categories":null,"content":"","date":1614729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614729600,"objectID":"5aa243caa8500a77bc268e15cde475a1","permalink":"https://alanjern.github.io/popular/nautilus21/","publishdate":"2021-03-03T00:00:00Z","relpermalink":"/popular/nautilus21/","section":"popular","summary":"Nautilus. March 3, 2021.","tags":null,"title":"The intelligent life of droids","type":"popular"},{"authors":null,"categories":null,"content":"","date":1602547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602547200,"objectID":"9e0dfcefd4571395caf43ab94151b034","permalink":"https://alanjern.github.io/popular/psyche20/","publishdate":"2020-10-13T00:00:00Z","relpermalink":"/popular/psyche20/","section":"popular","summary":"Psyche. October 13, 2020.","tags":null,"title":"Effective altruism is logical, but too unnatural to catch on","type":"popular"},{"authors":null,"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"e381f944de944cb03c907850016c636b","permalink":"https://alanjern.github.io/popular/vox20/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/popular/vox20/","section":"popular","summary":"Vox. September 1, 2020.","tags":null,"title":"Covid-19 death skepticism, explained by a cognitive scientist","type":"popular"},{"authors":null,"categories":null,"content":"","date":1588204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588204800,"objectID":"2d46052ee49e00b668f7d0fc1ae1ad7a","permalink":"https://alanjern.github.io/popular/culturico20/","publishdate":"2020-04-30T00:00:00Z","relpermalink":"/popular/culturico20/","section":"popular","summary":"culturico. April 30, 2020.","tags":null,"title":"HBO's My Brilliant Friend: a glance into the reality of child prodigies","type":"popular"},{"authors":null,"categories":null,"content":"","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"b3b961a1b2785093e871c763a67fc85d","permalink":"https://alanjern.github.io/popular/cogbites20/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/popular/cogbites20/","section":"popular","summary":"cogbites. March 16, 2020.","tags":null,"title":"Could stabbing yourself help you remember? The cognitive psychology behind a scene from Netflix‚Äôs Altered Carbon","type":"popular"},{"authors":null,"categories":null,"content":"","date":1583452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583452800,"objectID":"f7289c1ff66209e89b4078257f43efca","permalink":"https://alanjern.github.io/popular/areo20/","publishdate":"2020-03-06T00:00:00Z","relpermalink":"/popular/areo20/","section":"popular","summary":"Areo. March 6, 2020.","tags":null,"title":"There are no \"good\" or \"bad\" people","type":"popular"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://alanjern.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["A. Jern"],"categories":null,"content":"Commentary on Boyer \u0026amp; Petersen\n","date":1535587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535587200,"objectID":"4e3899e5c2871b11a447b5ea21cf8702","permalink":"https://alanjern.github.io/publication/jern-18/","publishdate":"2021-06-10T20:29:59.992403Z","relpermalink":"/publication/jern-18/","section":"publication","summary":"Commentary on Boyer \u0026amp; Petersen","tags":null,"title":"People are intuitive economists under the right conditions","type":"publication"},{"authors":["A. Jern"],"categories":null,"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"649ca3b3b4a87f6a5c08db81ad60a8b9","permalink":"https://alanjern.github.io/publication/jern-18-b/","publishdate":"2021-06-10T20:29:59.991993Z","relpermalink":"/publication/jern-18-b/","section":"publication","summary":"","tags":null,"title":"A preliminary study of the educational benefits of conducting replications in the classroom","type":"publication"},{"authors":["A. Jern","C. G. Lucas","C. Kemp"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"28535ef240e0e1c3121270e8396db059","permalink":"https://alanjern.github.io/publication/jern-et-al-17/","publishdate":"2021-06-10T20:29:59.991578Z","relpermalink":"/publication/jern-et-al-17/","section":"publication","summary":"","tags":null,"title":"People learn other people's preferences through inverse decision-making","type":"publication"},{"authors":null,"categories":null,"content":"","date":1478131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478131200,"objectID":"6c2240b46aa40e5c6cff7e863cdf6842","permalink":"https://alanjern.github.io/popular/conversation16b/","publishdate":"2016-11-03T00:00:00Z","relpermalink":"/popular/conversation16b/","section":"popular","summary":"The Conversation. November 3, 2016.","tags":null,"title":"What HBO's Westworld gets wrong (and right) about human nature","type":"popular"},{"authors":null,"categories":null,"content":"","date":1468454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1468454400,"objectID":"9ad1faa05b0c90c45ea4c8796fcdc113","permalink":"https://alanjern.github.io/popular/conversation16a/","publishdate":"2016-07-14T00:00:00Z","relpermalink":"/popular/conversation16a/","section":"popular","summary":"The Conversation. July 14, 2016.","tags":null,"title":"Enough with the spoiler alerts! Plot spoilers often increase enjoyment","type":"popular"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://alanjern.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://alanjern.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built *project* page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["A. Jern","C. Kemp"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"0513346b904d0d8948b72274454f23b9","permalink":"https://alanjern.github.io/publication/jern-kemp-15/","publishdate":"2021-06-10T20:29:59.99117Z","relpermalink":"/publication/jern-kemp-15/","section":"publication","summary":"","tags":null,"title":"A decision network account of reasoning about other people's choices","type":"publication"},{"authors":["AJ Piergiovanni","A. Jern"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"0677a55ce3ca47f335f6bcc0bf279443","permalink":"https://alanjern.github.io/publication/piergiovanni-jern-15/","publishdate":"2021-06-11T21:40:27.866452Z","relpermalink":"/publication/piergiovanni-jern-15/","section":"publication","summary":"","tags":null,"title":"Computational principles underlying people's behavior explanations","type":"publication"},{"authors":["A. Jern","C. Kemp"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"9015d45161e6d476abe2a898135a9762","permalink":"https://alanjern.github.io/publication/jern-kemp-14/","publishdate":"2021-06-11T21:40:27.863741Z","relpermalink":"/publication/jern-kemp-14/","section":"publication","summary":"","tags":null,"title":"Reasoning about social choices and social relationships","type":"publication"},{"authors":["C. Kemp","A. Jern"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"a13372d73b58d76986d36c35f78aad52","permalink":"https://alanjern.github.io/publication/kemp-jern-14/","publishdate":"2021-06-10T20:29:59.990296Z","relpermalink":"/publication/kemp-jern-14/","section":"publication","summary":"","tags":null,"title":"A taxonomy of inductive problems","type":"publication"},{"authors":["A. Jern","K. K. Chang","C. Kemp"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"5f7d10bae4f0c730577e7a8862705264","permalink":"https://alanjern.github.io/publication/jern-et-al-14/","publishdate":"2021-06-10T20:29:59.990742Z","relpermalink":"/publication/jern-et-al-14/","section":"publication","summary":"","tags":null,"title":"Belief polarization is not always rational","type":"publication"},{"authors":["A. Jern","C. Kemp"],"categories":null,"content":"","date":1359676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1359676800,"objectID":"621b1c203bea6e2d100692253a2b024e","permalink":"https://alanjern.github.io/publication/jern-kemp-13/","publishdate":"2021-06-10T20:29:59.98936Z","relpermalink":"/publication/jern-kemp-13/","section":"publication","summary":"","tags":null,"title":"A probabilistic account of exemplar and category generation","type":"publication"},{"authors":["A. Jern","C. G. Lucas","C. Kemp"],"categories":null,"content":"","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1322697600,"objectID":"0821c89ad95d6b6ae5d7cdbb4f1c4dbe","permalink":"https://alanjern.github.io/publication/jern-et-al-11-preferences/","publishdate":"2021-06-11T21:40:27.861594Z","relpermalink":"/publication/jern-et-al-11-preferences/","section":"publication","summary":"","tags":null,"title":"Evaluating the inverse decision-making approach to preference learning","type":"publication"},{"authors":["A. Jern","C. Kemp"],"categories":null,"content":"","date":1309478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309478400,"objectID":"edf2de4bcfbea1125cf803624bace88e","permalink":"https://alanjern.github.io/publication/jern-kemp-11-a/","publishdate":"2021-06-11T21:40:27.862637Z","relpermalink":"/publication/jern-kemp-11-a/","section":"publication","summary":"","tags":null,"title":"Capturing mental state reasoning with influence diagrams","type":"publication"},{"authors":["C. Kemp","F. Han","A. Jern"],"categories":null,"content":"","date":1309478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309478400,"objectID":"84bde1d785d6fd893c7aa6b2c627441b","permalink":"https://alanjern.github.io/publication/kemp-et-al-11/","publishdate":"2021-06-11T21:40:27.864784Z","relpermalink":"/publication/kemp-et-al-11/","section":"publication","summary":"","tags":null,"title":"Concept learning and modal reasoning","type":"publication"},{"authors":["A. Jern","C. Kemp"],"categories":null,"content":"","date":1309478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309478400,"objectID":"d015a7e15fa23e7ea9052af94a15b844","permalink":"https://alanjern.github.io/publication/jern-kemp-11-b/","publishdate":"2021-06-11T21:40:27.863217Z","relpermalink":"/publication/jern-kemp-11-b/","section":"publication","summary":"","tags":null,"title":"Decision factors that support preference learning","type":"publication"},{"authors":["C. Kemp","A. Jern"],"categories":null,"content":"","date":1259625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1259625600,"objectID":"0fd8d61be6c782675f936c0dc82a443e","permalink":"https://alanjern.github.io/publication/kemp-jern-09-relationallearning/","publishdate":"2021-06-11T21:40:27.865345Z","relpermalink":"/publication/kemp-jern-09-relationallearning/","section":"publication","summary":"","tags":null,"title":"Abstraction and relational learning","type":"publication"},{"authors":["A. Jern","K. K. Chang","C. Kemp"],"categories":null,"content":"","date":1259625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1259625600,"objectID":"2f060b31257edc7a8598b3eca1f99db1","permalink":"https://alanjern.github.io/publication/jern-et-al-09/","publishdate":"2021-06-11T21:40:27.860673Z","relpermalink":"/publication/jern-et-al-09/","section":"publication","summary":"","tags":null,"title":"Bayesian belief polarization","type":"publication"},{"authors":["C. Kemp","A. Jern","F. Xu"],"categories":null,"content":"","date":1259625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1259625600,"objectID":"836aa9f5dfb237fd664873a2550d69b7","permalink":"https://alanjern.github.io/publication/kemp-et-al-09/","publishdate":"2021-06-11T21:40:27.864274Z","relpermalink":"/publication/kemp-et-al-09/","section":"publication","summary":"","tags":null,"title":"Object discovery and identification","type":"publication"},{"authors":["C. Kemp","A. Jern"],"categories":null,"content":"","date":1246406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1246406400,"objectID":"694f9a584cf73c56f8bfe174d4e39e6b","permalink":"https://alanjern.github.io/publication/kemp-jern-09-taxonomy/","publishdate":"2021-06-11T21:40:27.865902Z","relpermalink":"/publication/kemp-jern-09-taxonomy/","section":"publication","summary":"","tags":null,"title":"A taxonomy of inductive problems","type":"publication"},{"authors":["A. Jern","C. Kemp"],"categories":null,"content":"","date":1246406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1246406400,"objectID":"2f2faf1415e43cfca9fb4283e19c220e","permalink":"https://alanjern.github.io/publication/jern-kemp-09/","publishdate":"2021-06-11T21:40:27.862126Z","relpermalink":"/publication/jern-kemp-09/","section":"publication","summary":"","tags":null,"title":"Category generation","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://alanjern.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]